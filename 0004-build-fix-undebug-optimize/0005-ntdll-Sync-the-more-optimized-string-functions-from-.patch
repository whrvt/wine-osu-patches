From a55deb30503f7f9a3ee3cadf2749b8755ec5a541 Mon Sep 17 00:00:00 2001
From: William Horvath <william@horvath.blog>
Date: Thu, 2 Jan 2025 11:42:57 -0800
Subject: [PATCH] ntdll: Sync the more optimized string functions from msvcrt.

---
 dlls/ntdll/string.c | 630 ++++++++++++++++++++++++++++++++++++++------
 1 file changed, 551 insertions(+), 79 deletions(-)

diff --git a/dlls/ntdll/string.c b/dlls/ntdll/string.c
index d334c169332..28f14cc9afe 100644
--- a/dlls/ntdll/string.c
+++ b/dlls/ntdll/string.c
@@ -32,8 +32,15 @@
 #include "winbase.h"
 #include "winnls.h"
 #include "winternl.h"
+#include "wine/asm.h"
+#include "ddk/wdm.h"
 #include "ntdll_misc.h"
 
+#if defined(__clang__) && (defined(__MINGW64__) || defined(__MINGW32__))
+#define __mingw_aligned_malloc _aligned_malloc
+#define __mingw_aligned_free _aligned_free
+#endif
+#include <immintrin.h>
 
 /* same as wctypes except for TAB, which doesn't have C1_BLANK for some reason... */
 static const unsigned short ctypes[257] =
@@ -79,128 +82,500 @@ void * __cdecl memchr( const void *ptr, int c, size_t n )
 }
 
 
-/*********************************************************************
- *                  memcmp   (NTDLL.@)
- */
-int __cdecl memcmp( const void *ptr1, const void *ptr2, size_t n )
+static inline int memcmp_bytes(const void *ptr1, const void *ptr2, size_t n)
 {
     const unsigned char *p1, *p2;
 
     for (p1 = ptr1, p2 = ptr2; n; n--, p1++, p2++)
     {
-        if (*p1 < *p2) return -1;
-        if (*p1 > *p2) return 1;
+        if (*p1 != *p2)
+            return *p1 > *p2 ? 1 : -1;
     }
     return 0;
 }
 
-
-static FORCEINLINE void memmove_unaligned_24( char *d, const char *s, size_t n )
+static inline int memcmp_blocks(const void *ptr1, const void *ptr2, size_t size)
 {
     typedef uint64_t DECLSPEC_ALIGN(1) unaligned_ui64;
-    typedef uint32_t DECLSPEC_ALIGN(1) unaligned_ui32;
-    typedef uint16_t DECLSPEC_ALIGN(1) unaligned_ui16;
-    uint64_t tmp0, tmp1, tmpn;
 
-    if (n >= 16)
+    const uint64_t *p1 = ptr1;
+    const unaligned_ui64 *p2 = ptr2;
+    size_t remainder = size & (sizeof(uint64_t) - 1);
+    size_t block_count = size / sizeof(uint64_t);
+
+    while (block_count)
+    {
+        if (*p1 != *p2)
+            return memcmp_bytes(p1, p2, sizeof(uint64_t));
+
+        p1++;
+        p2++;
+        block_count--;
+    }
+
+    return memcmp_bytes(p1, p2, remainder);
+}
+
+/*********************************************************************
+ *                  memcmp (NTDLL.@)
+ */
+int __cdecl memcmp(const void *ptr1, const void *ptr2, size_t n)
+{
+    const unsigned char *p1 = ptr1, *p2 = ptr2;
+    size_t align;
+    int result;
+
+    if (n < sizeof(uint64_t))
+        return memcmp_bytes(p1, p2, n);
+
+    align = -(size_t)p1 & (sizeof(uint64_t) - 1);
+
+    if ((result = memcmp_bytes(p1, p2, align)))
+        return result;
+
+    p1 += align;
+    p2 += align;
+    n  -= align;
+
+    return memcmp_blocks(p1, p2, n);
+}
+
+#if defined(__i386__) || (defined(__x86_64__) && !defined(__arm64ec__))
+
+#if __has_builtin(__cpuidex) || (defined(_MSC_VER) && !defined(__clang__))
+void __cpuidex(int info[4], int ax, int cx);
+#pragma intrinsic(__cpuidex)
+#else
+static FORCEINLINE void __cpuidex(int info[4], int ax, int cx)
+{
+  __asm__ ("cpuid" : "=a"(info[0]), "=b" (info[1]), "=c"(info[2]), "=d"(info[3]) : "a"(ax), "c"(cx));
+}
+#endif
+
+#if __has_builtin(__cpuid) || (defined(_MSC_VER) && !defined(__clang__))
+void __cpuid(int info[4], int ax);
+#pragma intrinsic(__cpuid)
+#else
+static FORCEINLINE void __cpuid(int info[4], int ax)
+{
+    return __cpuidex(info, ax, 0);
+}
+#endif
+
+static FORCEINLINE BOOL erms_supported( void )
+{
+    static BOOL supported;
+    if (!supported)
+    {
+        int regs[4];
+        __cpuid(regs, 0);
+        if ((regs[1] >> 9) & 1)
+            supported = 1;
+        else
+            supported = -1;
+    }
+    return supported > 0;
+}
+
+static FORCEINLINE BOOL avx_supported( void )
+{
+    static BOOL supported;
+    if (!supported)
     {
-        tmp0 = *(unaligned_ui64 *)s;
-        tmp1 = *(unaligned_ui64 *)(s + 8);
-        tmpn = *(unaligned_ui64 *)(s + n - 8);
-        *(unaligned_ui64 *)d = tmp0;
-        *(unaligned_ui64 *)(d + 8) = tmp1;
-        *(unaligned_ui64 *)(d + n - 8) = tmpn;
+        int regs[4];
+        __cpuid(regs, 1);
+        if (user_shared_data->ProcessorFeatures[PF_XSAVE_ENABLED] && (regs[2] & (1 << 28)) && ((_xgetbv(0) & 0x6) == 0x6))
+            supported = 1;
+        else
+            supported = -1;
+    }
+    return supported > 0;
+}
+
+static FORCEINLINE BOOL sse2_supported( void )
+{
+    static BOOL supported;
+    if (!supported)
+    {
+        if (user_shared_data->ProcessorFeatures[PF_XMMI64_INSTRUCTIONS_AVAILABLE])
+            supported = 1;
+        else
+            supported = -1;
     }
-    else if (n >= 8)
+    return supported > 0;
+}
+
+#else
+#define erms_supported() 0
+#define sse2_supported() 0
+#define avx_supported()  0
+#endif
+
+#define likely(x) __builtin_expect(x, 1)
+#define unlikely(x) __builtin_expect(x, 0)
+
+#define MEMMOVEV_UNALIGNED_DECLARE(name, type, size, loadu, storeu) \
+static FORCEINLINE void memmove_ ## name ## _unaligned(char *d, const char *s, size_t n) \
+{ \
+    type tmp0, tmp1, tmp2, tmp3, tmp4, tmp5; \
+    if (unlikely(n > 4 * size)) \
+    { \
+        tmp0 = loadu((type *)(s + 0 * size)); \
+        tmp1 = loadu((type *)(s + 1 * size)); \
+        tmp2 = loadu((type *)(s + 2 * size)); \
+        tmp3 = loadu((type *)(s + n - 3 * size)); \
+        tmp4 = loadu((type *)(s + n - 2 * size)); \
+        tmp5 = loadu((type *)(s + n - 1 * size)); \
+        storeu((type *)(d + 0 * size), tmp0); \
+        storeu((type *)(d + 1 * size), tmp1); \
+        storeu((type *)(d + 2 * size), tmp2); \
+        storeu((type *)(d + n - 3 * size), tmp3); \
+        storeu((type *)(d + n - 2 * size), tmp4); \
+        storeu((type *)(d + n - 1 * size), tmp5); \
+    } \
+    else if (unlikely(n > size * 2)) \
+    { \
+        tmp0 = loadu((type *)(s + 0 * size)); \
+        tmp1 = loadu((type *)(s + 1 * size)); \
+        tmp2 = loadu((type *)(s + n - 2 * size)); \
+        tmp3 = loadu((type *)(s + n - 1 * size)); \
+        storeu((type *)(d + 0 * size), tmp0); \
+        storeu((type *)(d + 1 * size), tmp1); \
+        storeu((type *)(d + n - 2 * size), tmp2); \
+        storeu((type *)(d + n - 1 * size), tmp3); \
+    } \
+    else if (unlikely(n > size)) \
+    { \
+        tmp0 = loadu((type *)(s + 0 * size)); \
+        tmp1 = loadu((type *)(s + n - 1 * size)); \
+        storeu((type *)(d + 0 * size), tmp0); \
+        storeu((type *)(d + n - 1 * size), tmp1); \
+    } \
+    else memmove_c_unaligned_32(d, s, n); \
+}
+
+#define MEMMOVEV_DECLARE(name, type, size, loadu, storeu, store) \
+static void *__cdecl memmove_ ## name(char *d, const char *s, size_t n) \
+{ \
+    if (likely(n <= 6 * size)) memmove_ ## name ## _unaligned(d, s, n); \
+    else if (d <= s) \
+    { \
+        type tmp0, tmp1, tmp2, tmp3, tmp4, tmp5; \
+        size_t k = (size - ((uintptr_t)d & (size - 1))); \
+        tmp0 = loadu((type *)s); \
+        tmp1 = loadu((type *)(s + k + 0 * size)); \
+        tmp2 = loadu((type *)(s + k + 1 * size)); \
+        tmp3 = loadu((type *)(s + k + 2 * size)); \
+        tmp4 = loadu((type *)(s + k + 3 * size)); \
+        tmp5 = loadu((type *)(s + k + 4 * size)); \
+        storeu((type *)d, tmp0); \
+        store((type *)(d + k + 0 * size), tmp1); \
+        store((type *)(d + k + 1 * size), tmp2); \
+        store((type *)(d + k + 2 * size), tmp3); \
+        store((type *)(d + k + 3 * size), tmp4); \
+        store((type *)(d + k + 4 * size), tmp5); \
+        k += 5 * size; d += k; s += k; n -= k; \
+        while (unlikely(n >= 12 * size)) \
+        { \
+            tmp0 = loadu((type *)(s + 0 * size)); \
+            tmp1 = loadu((type *)(s + 1 * size)); \
+            tmp2 = loadu((type *)(s + 2 * size)); \
+            tmp3 = loadu((type *)(s + 3 * size)); \
+            tmp4 = loadu((type *)(s + 4 * size)); \
+            tmp5 = loadu((type *)(s + 5 * size)); \
+            store((type *)(d + 0 * size), tmp0); \
+            store((type *)(d + 1 * size), tmp1); \
+            store((type *)(d + 2 * size), tmp2); \
+            store((type *)(d + 3 * size), tmp3); \
+            store((type *)(d + 4 * size), tmp4); \
+            store((type *)(d + 5 * size), tmp5); \
+            tmp0 = loadu((type *)(s +  6 * size)); \
+            tmp1 = loadu((type *)(s +  7 * size)); \
+            tmp2 = loadu((type *)(s +  8 * size)); \
+            tmp3 = loadu((type *)(s +  9 * size)); \
+            tmp4 = loadu((type *)(s + 10 * size)); \
+            tmp5 = loadu((type *)(s + 11 * size)); \
+            store((type *)(d +  6 * size), tmp0); \
+            store((type *)(d +  7 * size), tmp1); \
+            store((type *)(d +  8 * size), tmp2); \
+            store((type *)(d +  9 * size), tmp3); \
+            store((type *)(d + 10 * size), tmp4); \
+            store((type *)(d + 11 * size), tmp5); \
+            d += 12 * size; s += 12 * size; n -= 12 * size; k += 12 * size; \
+        } \
+        while (unlikely(n >= 6 * size)) \
+        { \
+            tmp0 = loadu((type *)(s + 0 * size)); \
+            tmp1 = loadu((type *)(s + 1 * size)); \
+            tmp2 = loadu((type *)(s + 2 * size)); \
+            tmp3 = loadu((type *)(s + 3 * size)); \
+            tmp4 = loadu((type *)(s + 4 * size)); \
+            tmp5 = loadu((type *)(s + 5 * size)); \
+            store((type *)(d + 0 * size), tmp0); \
+            store((type *)(d + 1 * size), tmp1); \
+            store((type *)(d + 2 * size), tmp2); \
+            store((type *)(d + 3 * size), tmp3); \
+            store((type *)(d + 4 * size), tmp4); \
+            store((type *)(d + 5 * size), tmp5); \
+            d += 6 * size; s += 6 * size; n -= 6 * size; k += 6 * size; \
+        } \
+        memmove_ ## name ## _unaligned(d, s, n); \
+        return d - k; \
+    } \
+    else \
+    { \
+        type tmp0, tmp1, tmp2, tmp3, tmp4, tmp5; \
+        size_t k = n - ((uintptr_t)(d + n) & (size - 1)); \
+        tmp0 = loadu((type *)(s + n - 1 * size)); \
+        tmp1 = loadu((type *)(s + k - 1 * size)); \
+        tmp2 = loadu((type *)(s + k - 2 * size)); \
+        tmp3 = loadu((type *)(s + k - 3 * size)); \
+        tmp4 = loadu((type *)(s + k - 4 * size)); \
+        tmp5 = loadu((type *)(s + k - 5 * size)); \
+        storeu((type *)(d + n - 1 * size), tmp0); \
+        store((type *)(d + k - 1 * size), tmp1); \
+        store((type *)(d + k - 2 * size), tmp2); \
+        store((type *)(d + k - 3 * size), tmp3); \
+        store((type *)(d + k - 4 * size), tmp4); \
+        store((type *)(d + k - 5 * size), tmp5); \
+        k -= 5 * size; \
+        while (unlikely(k >= 12 * size)) \
+        { \
+            tmp0 = loadu((type *)(s + k - 1 * size)); \
+            tmp1 = loadu((type *)(s + k - 2 * size)); \
+            tmp2 = loadu((type *)(s + k - 3 * size)); \
+            tmp3 = loadu((type *)(s + k - 4 * size)); \
+            tmp4 = loadu((type *)(s + k - 5 * size)); \
+            tmp5 = loadu((type *)(s + k - 6 * size)); \
+            store((type *)(d + k - 1 * size), tmp0); \
+            store((type *)(d + k - 2 * size), tmp1); \
+            store((type *)(d + k - 3 * size), tmp2); \
+            store((type *)(d + k - 4 * size), tmp3); \
+            store((type *)(d + k - 5 * size), tmp4); \
+            store((type *)(d + k - 6 * size), tmp5); \
+            tmp0 = loadu((type *)(s + k -  7 * size)); \
+            tmp1 = loadu((type *)(s + k -  8 * size)); \
+            tmp2 = loadu((type *)(s + k -  9 * size)); \
+            tmp3 = loadu((type *)(s + k - 10 * size)); \
+            tmp4 = loadu((type *)(s + k - 11 * size)); \
+            tmp5 = loadu((type *)(s + k - 12 * size)); \
+            store((type *)(d + k -  7 * size), tmp0); \
+            store((type *)(d + k -  8 * size), tmp1); \
+            store((type *)(d + k -  9 * size), tmp2); \
+            store((type *)(d + k - 10 * size), tmp3); \
+            store((type *)(d + k - 11 * size), tmp4); \
+            store((type *)(d + k - 12 * size), tmp5); \
+            k -= 12 * size; \
+        } \
+        while (unlikely(k >= 6 * size)) \
+        { \
+            tmp0 = loadu((type *)(s + k - 1 * size)); \
+            tmp1 = loadu((type *)(s + k - 2 * size)); \
+            tmp2 = loadu((type *)(s + k - 3 * size)); \
+            tmp3 = loadu((type *)(s + k - 4 * size)); \
+            tmp4 = loadu((type *)(s + k - 5 * size)); \
+            tmp5 = loadu((type *)(s + k - 6 * size)); \
+            store((type *)(d + k - 1 * size), tmp0); \
+            store((type *)(d + k - 2 * size), tmp1); \
+            store((type *)(d + k - 3 * size), tmp2); \
+            store((type *)(d + k - 4 * size), tmp3); \
+            store((type *)(d + k - 5 * size), tmp4); \
+            store((type *)(d + k - 6 * size), tmp5); \
+            k -= 6 * size; \
+        } \
+        memmove_ ## name ## _unaligned(d, s, k); \
+    } \
+    return d; \
+}
+
+static FORCEINLINE void __cdecl memmove_c_unaligned_32(char *d, const char *s, size_t n)
+{
+    uint64_t tmp0, tmp1, tmp2, tmpn;
+
+    if (unlikely(n >= 24))
     {
-        tmp0 = *(unaligned_ui64 *)s;
-        tmpn = *(unaligned_ui64 *)(s + n - 8);
-        *(unaligned_ui64 *)d = tmp0;
-        *(unaligned_ui64 *)(d + n - 8) = tmpn;
+        tmp0 = *(uint64_t *)s;
+        tmp1 = *(uint64_t *)(s + 8);
+        tmp2 = *(uint64_t *)(s + 16);
+        tmpn = *(uint64_t *)(s + n - 8);
+        *(uint64_t *)d = tmp0;
+        *(uint64_t *)(d + 8) = tmp1;
+        *(uint64_t *)(d + 16) = tmp2;
+        *(uint64_t *)(d + n - 8) = tmpn;
     }
-    else if (n >= 4)
+    else if (unlikely(n >= 16))
     {
-        tmp0 = *(unaligned_ui32 *)s;
-        tmpn = *(unaligned_ui32 *)(s + n - 4);
-        *(unaligned_ui32 *)d = tmp0;
-        *(unaligned_ui32 *)(d + n - 4) = tmpn;
+        tmp0 = *(uint64_t *)s;
+        tmp1 = *(uint64_t *)(s + 8);
+        tmpn = *(uint64_t *)(s + n - 8);
+        *(uint64_t *)d = tmp0;
+        *(uint64_t *)(d + 8) = tmp1;
+        *(uint64_t *)(d + n - 8) = tmpn;
     }
-    else if (n >= 2)
+    else if (unlikely(n >= 8))
     {
-        tmp0 = *(unaligned_ui16 *)s;
-        tmpn = *(unaligned_ui16 *)(s + n - 2);
-        *(unaligned_ui16 *)d = tmp0;
-        *(unaligned_ui16 *)(d + n - 2) = tmpn;
+        tmp0 = *(uint64_t *)s;
+        tmpn = *(uint64_t *)(s + n - 8);
+        *(uint64_t *)d = tmp0;
+        *(uint64_t *)(d + n - 8) = tmpn;
     }
-    else if (n >= 1)
+    else if (unlikely(n >= 4))
+    {
+        tmp0 = *(uint32_t *)s;
+        tmpn = *(uint32_t *)(s + n - 4);
+        *(uint32_t *)d = tmp0;
+        *(uint32_t *)(d + n - 4) = tmpn;
+    }
+    else if (unlikely(n >= 2))
+    {
+        tmp0 = *(uint16_t *)s;
+        tmpn = *(uint16_t *)(s + n - 2);
+        *(uint16_t *)d = tmp0;
+        *(uint16_t *)(d + n - 2) = tmpn;
+    }
+    else if (likely(n >= 1))
     {
         *(uint8_t *)d = *(uint8_t *)s;
     }
 }
 
-static FORCEINLINE void *memmove_unrolled( char *dst, const char *src, size_t n )
+static void *__cdecl memmove_c(char *d, const char *s, size_t n)
 {
-    typedef uint64_t DECLSPEC_ALIGN(1) unaligned_ui64;
-    uint64_t tmp0, tmp1, tmp2;
-    char *end;
-
-    if (n <= 24) memmove_unaligned_24( dst, src, n );
-    else if ((size_t)dst - (size_t)src >= n)
+    if (likely(n <= 32)) memmove_c_unaligned_32(d, s, n);
+    else if (d <= s)
     {
-        end = dst + n; src += n;
-        do
+        uint64_t tmp0, tmp1, tmp2;
+        size_t k = 0;
+        while (unlikely(n >= 48))
+        {
+            tmp0 = *(uint64_t *)(s +  0);
+            tmp1 = *(uint64_t *)(s +  8);
+            tmp2 = *(uint64_t *)(s + 16);
+            *(uint64_t*)(d +  0) = tmp0;
+            *(uint64_t*)(d +  8) = tmp1;
+            *(uint64_t*)(d + 16) = tmp2;
+            tmp0 = *(uint64_t *)(s + 24);
+            tmp1 = *(uint64_t *)(s + 32);
+            tmp2 = *(uint64_t *)(s + 40);
+            *(uint64_t*)(d + 24) = tmp0;
+            *(uint64_t*)(d + 32) = tmp1;
+            *(uint64_t*)(d + 40) = tmp2;
+            d += 48; s += 48; n -= 48; k += 48;
+        }
+        while (unlikely(n >= 24))
         {
-            tmp0 = *(unaligned_ui64 *)(src - n +  0);
-            tmp1 = *(unaligned_ui64 *)(src - n +  8);
-            tmp2 = *(unaligned_ui64 *)(src - n + 16);
-            *(unaligned_ui64*)(end - n +  0) = tmp0;
-            *(unaligned_ui64*)(end - n +  8) = tmp1;
-            *(unaligned_ui64*)(end - n + 16) = tmp2;
-            n -= 24;
+            tmp0 = *(uint64_t *)(s +  0);
+            tmp1 = *(uint64_t *)(s +  8);
+            tmp2 = *(uint64_t *)(s + 16);
+            *(uint64_t*)(d +  0) = tmp0;
+            *(uint64_t*)(d +  8) = tmp1;
+            *(uint64_t*)(d + 16) = tmp2;
+            d += 24; s += 24; n -= 24; k += 24;
         }
-        while (n >= 24);
-        memmove_unaligned_24( end - n, src - n, n );
+        memmove_c_unaligned_32(d, s, n);
+        return d - k;
     }
     else
     {
-        do
+        uint64_t tmp0, tmp1, tmp2;
+        size_t k = n;
+        while (unlikely(k >= 48))
         {
-            tmp0 = *(unaligned_ui64 *)(src + n -  8);
-            tmp1 = *(unaligned_ui64 *)(src + n - 16);
-            tmp2 = *(unaligned_ui64 *)(src + n - 24);
-            *(unaligned_ui64*)(dst + n -  8) = tmp0;
-            *(unaligned_ui64*)(dst + n - 16) = tmp1;
-            *(unaligned_ui64*)(dst + n - 24) = tmp2;
-            n -= 24;
+            tmp0 = *(uint64_t *)(s + k -  8);
+            tmp1 = *(uint64_t *)(s + k - 16);
+            tmp2 = *(uint64_t *)(s + k - 24);
+            *(uint64_t*)(d + k -  8) = tmp0;
+            *(uint64_t*)(d + k - 16) = tmp1;
+            *(uint64_t*)(d + k - 24) = tmp2;
+            tmp0 = *(uint64_t *)(s + k - 32);
+            tmp1 = *(uint64_t *)(s + k - 40);
+            tmp2 = *(uint64_t *)(s + k - 48);
+            *(uint64_t*)(d + k - 32) = tmp0;
+            *(uint64_t*)(d + k - 40) = tmp1;
+            *(uint64_t*)(d + k - 48) = tmp2;
+            k -= 48;
         }
-        while (n >= 24);
-        memmove_unaligned_24( dst, src, n );
+        while (unlikely(k >= 24))
+        {
+            tmp0 = *(uint64_t *)(s + k -  8);
+            tmp1 = *(uint64_t *)(s + k - 16);
+            tmp2 = *(uint64_t *)(s + k - 24);
+            *(uint64_t*)(d + k -  8) = tmp0;
+            *(uint64_t*)(d + k - 16) = tmp1;
+            *(uint64_t*)(d + k - 24) = tmp2;
+            k -= 24;
+        }
+        memmove_c_unaligned_32(d, s, k);
     }
-    return dst;
+    return d;
 }
 
+#ifndef __SSE2__
+#ifdef __clang__
+#pragma clang attribute push (__attribute__((target("sse2"))), apply_to=function)
+#else
+#pragma GCC push_options
+#pragma GCC target("sse2")
+#endif
+#define __DISABLE_SSE2__
+#endif /* __SSE2__ */
+
+MEMMOVEV_UNALIGNED_DECLARE(sse2, __m128i, 16, _mm_loadu_si128, _mm_storeu_si128)
+MEMMOVEV_DECLARE(sse2, __m128i, 16, _mm_loadu_si128, _mm_storeu_si128, _mm_store_si128)
+
+#ifdef __DISABLE_SSE2__
+#ifdef __clang__
+#pragma clang attribute pop
+#else
+#pragma GCC pop_options
+#endif
+#undef __DISABLE_SSE2__
+#endif /* __DISABLE_SSE2__ */
+
+#ifndef __AVX__
+#ifdef __clang__
+#pragma clang attribute push (__attribute__((target("avx"))), apply_to=function)
+#else
+#pragma GCC push_options
+#pragma GCC target("avx")
+#endif
+#define __DISABLE_AVX__
+#endif /* __AVX__ */
+
+MEMMOVEV_UNALIGNED_DECLARE(avx, __m256i, 32, _mm256_loadu_si256, _mm256_storeu_si256)
+MEMMOVEV_DECLARE(avx, __m256i, 32, _mm256_loadu_si256, _mm256_storeu_si256, _mm256_store_si256)
+
+#ifdef __DISABLE_AVX__
+#undef __DISABLE_AVX__
+#ifdef __clang__
+#pragma clang attribute pop
+#else
+#pragma GCC pop_options
+#endif
+#endif /* __DISABLE_AVX__ */
 
 /*********************************************************************
- *                  memcpy   (NTDLL.@)
- *
- * NOTES
- *  Behaves like memmove.
+ *                  memmove (NTDLL.@)
  */
-void * __cdecl memcpy( void *dst, const void *src, size_t n )
+void *__cdecl memmove(void *dst, const void *src, size_t n)
 {
-    return memmove_unrolled( dst, src, n );
+    if (unlikely(n < 32)) { memmove_c_unaligned_32(dst, src, n); return dst; }
+    if (likely(avx_supported())) return memmove_avx(dst, src, n);
+    if (likely(sse2_supported())) return memmove_sse2(dst, src, n);
+    return memmove_c(dst, src, n);
 }
 
-
 /*********************************************************************
- *                  memmove   (NTDLL.@)
+ *                  memcpy   (NTDLL.@)
  */
-void * __cdecl memmove( void *dst, const void *src, size_t n )
+void *__cdecl memcpy(void *dst, const void *src, size_t n)
 {
-    return memmove_unrolled( dst, src, n );
+    if (unlikely(n < 32)) { memmove_c_unaligned_32(dst, src, n); return dst; }
+    if (likely(avx_supported())) return memmove_avx(dst, src, n);
+    if (likely(sse2_supported())) return memmove_sse2(dst, src, n);
+    return memmove_c(dst, src, n);
 }
 
-
 /*********************************************************************
  *		memcpy_s (MSVCRT.@)
  */
@@ -250,10 +625,83 @@ static inline void memset_aligned_32( unsigned char *d, uint64_t v, size_t n )
     }
 }
 
-/*********************************************************************
- *                  memset   (NTDLL.@)
- */
-void *__cdecl memset( void *dst, int c, size_t n )
+#if defined(__i386__) || (defined(__x86_64__) && !defined(__arm64ec__))
+
+#ifdef __i386__
+#define DEST_REG "%edi"
+#define LEN_REG "%ecx"
+#define VAL_REG "%eax"
+
+#define MEMSET_INIT \
+    "movl " DEST_REG ", %edx\n\t" \
+    "movl 4(%esp), " DEST_REG "\n\t" \
+    "movl 8(%esp), " VAL_REG "\n\t" \
+    "movl 12(%esp), " LEN_REG "\n\t"
+
+#define MEMSET_RET \
+    "movl %edx, " DEST_REG "\n\t" \
+    "ret"
+
+#else
+
+#define DEST_REG "%rdi"
+#define LEN_REG "%rcx"
+#define VAL_REG "%eax"
+
+#define MEMSET_INIT \
+    "movq " DEST_REG ", %r9\n\t" \
+    "movq %rcx, " DEST_REG "\n\t" \
+    "movl %edx, " VAL_REG "\n\t" \
+    "movq %r8, " LEN_REG "\n\t"
+
+#define MEMSET_RET \
+    "movq %r9, " DEST_REG "\n\t" \
+    "ret"
+
+#endif
+
+void __cdecl erms_memset_aligned_32(unsigned char *d, unsigned int c, size_t n);
+__ASM_GLOBAL_FUNC( erms_memset_aligned_32,
+        MEMSET_INIT
+        "rep\n\t"
+        "stosb\n\t"
+        MEMSET_RET )
+
+void __cdecl sse2_memset_aligned_32(unsigned char *d, unsigned int c, size_t n);
+__ASM_GLOBAL_FUNC( sse2_memset_aligned_32,
+        MEMSET_INIT
+        "movd " VAL_REG ", %xmm0\n\t"
+        "pshufd $0, %xmm0, %xmm0\n\t"
+        "test $0x20, " LEN_REG "\n\t"
+        "je 1f\n\t"
+        "add $0x20, " DEST_REG "\n\t"
+        "sub $0x20, " LEN_REG "\n\t"
+        "movdqa %xmm0, -0x20(" DEST_REG ")\n\t"
+        "movdqa %xmm0, -0x10(" DEST_REG ")\n\t"
+        "je 2f\n\t"
+        "1:\n\t"
+        "add $0x40, " DEST_REG "\n\t"
+        "sub $0x40, " LEN_REG "\n\t"
+        "movdqa %xmm0, -0x40(" DEST_REG ")\n\t"
+        "movdqa %xmm0, -0x30(" DEST_REG ")\n\t"
+        "movdqa %xmm0, -0x20(" DEST_REG ")\n\t"
+        "movdqa %xmm0, -0x10(" DEST_REG ")\n\t"
+        "ja 1b\n\t"
+        "2:\n\t"
+        MEMSET_RET )
+
+#undef MEMSET_INIT
+#undef MEMSET_RET
+#undef DEST_REG
+#undef LEN_REG
+#undef VAL_REG
+
+#endif
+
+/*********************************************************************
+ *		    memset (NTDLL.@)
+ */
+void *__cdecl memset(void *dst, int c, size_t n)
 {
     typedef uint64_t DECLSPEC_ALIGN(1) unaligned_ui64;
     typedef uint32_t DECLSPEC_ALIGN(1) unaligned_ui32;
@@ -277,8 +725,27 @@ void *__cdecl memset( void *dst, int c, size_t n )
         if (n <= 64) return dst;
 
         n = (n - a) & ~0x1f;
-        memset_aligned_32( d + a, v, n );
+#if defined(__i386__) || defined(__x86_64__)
+        if (n >= 2048 && erms_supported)
+        {
+            erms_memset_aligned_32(d + a, v, n);
+            return dst;
+        }
+#ifdef __x86_64__
+        sse2_memset_aligned_32(d + a, v, n);
         return dst;
+#else
+        if (sse2_supported())
+        {
+            sse2_memset_aligned_32(d + a, v, n);
+            return dst;
+        }
+#endif
+#endif
+#ifndef __x86_64__
+        memset_aligned_32(d + a, v, n);
+        return dst;
+#endif
     }
     if (n >= 8)
     {
@@ -306,6 +773,11 @@ void *__cdecl memset( void *dst, int c, size_t n )
     return dst;
 }
 
+#undef MEMMOVEV_DECLARE
+#undef MEMMOVEV_UNALIGNED_DECLARE
+#undef likely
+#undef unlikely
+
 
 /******************************************************************************
  *                  RtlCopyMemory   (NTDLL.@)
-- 
2.47.1

