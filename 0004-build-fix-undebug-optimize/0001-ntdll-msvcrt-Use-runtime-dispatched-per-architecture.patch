From 939748f3060173555f8c5b17fc021bfd20c70006 Mon Sep 17 00:00:00 2001
From: William Horvath <william@horvath.blog>
Date: Tue, 11 Feb 2025 11:20:54 -0800
Subject: [PATCH] ntdll, msvcrt: Use runtime-dispatched
 per-architecture-optimized code for string functions.

---
 dlls/msvcrt/string.c | 622 +++++++++++++++----------------------------
 dlls/ntdll/string.c  | 321 +++++++++++++++-------
 2 files changed, 445 insertions(+), 498 deletions(-)

diff --git a/dlls/msvcrt/string.c b/dlls/msvcrt/string.c
index 90538533d95..34a64a722bf 100644
--- a/dlls/msvcrt/string.c
+++ b/dlls/msvcrt/string.c
@@ -2786,374 +2786,170 @@ int __cdecl memcmp(const void *ptr1, const void *ptr2, size_t n)
     return memcmp_blocks(p1, p2, n);
 }
 
-#if defined(__i386__) || (defined(__x86_64__) && !defined(__arm64ec__))
-
-#ifdef __i386__
-
-#define DEST_REG "%edi"
-#define SRC_REG "%esi"
-#define LEN_REG "%ecx"
-#define TMP_REG "%edx"
-
-#define MEMMOVE_INIT \
-    "pushl " SRC_REG "\n\t" \
-    __ASM_CFI(".cfi_adjust_cfa_offset 4\n\t") \
-    "pushl " DEST_REG "\n\t" \
-    __ASM_CFI(".cfi_adjust_cfa_offset 4\n\t") \
-    "movl 12(%esp), " DEST_REG "\n\t" \
-    "movl 16(%esp), " SRC_REG "\n\t" \
-    "movl 20(%esp), " LEN_REG "\n\t"
-
-#define MEMMOVE_CLEANUP \
-    "movl 12(%esp), %eax\n\t" \
-    "popl " DEST_REG "\n\t" \
-    __ASM_CFI(".cfi_adjust_cfa_offset -4\n\t") \
-    "popl " SRC_REG "\n\t" \
-    __ASM_CFI(".cfi_adjust_cfa_offset -4\n\t")
+#ifndef likely
+#define likely(x) __builtin_expect(!!(x), 1)
+#endif
+#ifndef unlikely
+#define unlikely(x) __builtin_expect(!!(x), 0)
+#endif
 
+#ifdef __clang__
+#define NOBUILTIN __attribute__((no_builtin("memcpy", "memmove", "memset", "memcmp")))
 #else
-
-#define DEST_REG "%rdi"
-#define SRC_REG "%rsi"
-#define LEN_REG "%r8"
-#define TMP_REG "%r9"
-
-#define MEMMOVE_INIT \
-    "pushq " SRC_REG "\n\t" \
-    __ASM_SEH(".seh_pushreg " SRC_REG "\n\t") \
-    __ASM_CFI(".cfi_adjust_cfa_offset 8\n\t") \
-    "pushq " DEST_REG "\n\t" \
-    __ASM_SEH(".seh_pushreg " DEST_REG "\n\t") \
-    __ASM_SEH(".seh_endprologue\n\t") \
-    __ASM_CFI(".cfi_adjust_cfa_offset 8\n\t") \
-    "movq %rcx, " DEST_REG "\n\t" \
-    "movq %rdx, " SRC_REG "\n\t"
-
-#define MEMMOVE_CLEANUP \
-    "movq %rcx, %rax\n\t" \
-    "popq " DEST_REG "\n\t" \
-    __ASM_CFI(".cfi_adjust_cfa_offset -8\n\t") \
-    "popq " SRC_REG "\n\t" \
-    __ASM_CFI(".cfi_adjust_cfa_offset -8\n\t")
+#define NOBUILTIN __attribute__((__optimize__("-fno-tree-loop-distribute-patterns")))
 #endif
 
-void * __cdecl sse2_memmove(void *dst, const void *src, size_t n);
-__ASM_GLOBAL_FUNC( sse2_memmove,
-        MEMMOVE_INIT
-        "mov " DEST_REG ", " TMP_REG "\n\t" /* check copying direction */
-        "sub " SRC_REG ", " TMP_REG "\n\t"
-        "cmp " LEN_REG ", " TMP_REG "\n\t"
-        "jb copy_bwd\n\t"
-        /* copy forwards */
-        "cmp $4, " LEN_REG "\n\t" /* 4-bytes align */
-        "jb copy_fwd3\n\t"
-        "mov " DEST_REG ", " TMP_REG "\n\t"
-        "shr $1, " TMP_REG "\n\t"
-        "jnc 1f\n\t"
-        "movsb\n\t"
-        "dec " LEN_REG "\n\t"
-        "inc " TMP_REG "\n\t"
-        "1:\n\t"
-        "shr $1, " TMP_REG "\n\t"
-        "jnc 1f\n\t"
-        "movsw\n\t"
-        "sub $2, " LEN_REG "\n\t"
-        "inc " TMP_REG "\n\t"
-        "1:\n\t" /* 16-bytes align */
-        "cmp $16, " LEN_REG "\n\t"
-        "jb copy_fwd15\n\t"
-        "shr $1, " TMP_REG "\n\t"
-        "jnc 1f\n\t"
-        "movsl\n\t"
-        "sub $4, " LEN_REG "\n\t"
-        "inc " TMP_REG "\n\t"
-        "1:\n\t"
-        "shr $1, " TMP_REG "\n\t"
-        "jnc 1f\n\t"
-        "movsl\n\t"
-        "movsl\n\t"
-        "sub $8, " LEN_REG "\n\t"
-        "1:\n\t"
-        "cmp $64, " LEN_REG "\n\t"
-        "jb copy_fwd63\n\t"
-        "1:\n\t" /* copy 64-bytes blocks in loop, dest 16-bytes aligned */
-        "movdqu 0x00(" SRC_REG "), %xmm0\n\t"
-        "movdqu 0x10(" SRC_REG "), %xmm1\n\t"
-        "movdqu 0x20(" SRC_REG "), %xmm2\n\t"
-        "movdqu 0x30(" SRC_REG "), %xmm3\n\t"
-        "movdqa %xmm0, 0x00(" DEST_REG ")\n\t"
-        "movdqa %xmm1, 0x10(" DEST_REG ")\n\t"
-        "movdqa %xmm2, 0x20(" DEST_REG ")\n\t"
-        "movdqa %xmm3, 0x30(" DEST_REG ")\n\t"
-        "add $64, " SRC_REG "\n\t"
-        "add $64, " DEST_REG "\n\t"
-        "sub $64, " LEN_REG "\n\t"
-        "cmp $64, " LEN_REG "\n\t"
-        "jae 1b\n\t"
-        "copy_fwd63:\n\t" /* copy last 63 bytes, dest 16-bytes aligned */
-        "mov " LEN_REG ", " TMP_REG "\n\t"
-        "and $15, " LEN_REG "\n\t"
-        "shr $5, " TMP_REG "\n\t"
-        "jnc 1f\n\t"
-        "movdqu 0(" SRC_REG "), %xmm0\n\t"
-        "movdqa %xmm0, 0(" DEST_REG ")\n\t"
-        "add $16, " SRC_REG "\n\t"
-        "add $16, " DEST_REG "\n\t"
-        "1:\n\t"
-        "shr $1, " TMP_REG "\n\t"
-        "jnc copy_fwd15\n\t"
-        "movdqu 0x00(" SRC_REG "), %xmm0\n\t"
-        "movdqu 0x10(" SRC_REG "), %xmm1\n\t"
-        "movdqa %xmm0, 0x00(" DEST_REG ")\n\t"
-        "movdqa %xmm1, 0x10(" DEST_REG ")\n\t"
-        "add $32, " SRC_REG "\n\t"
-        "add $32, " DEST_REG "\n\t"
-        "copy_fwd15:\n\t" /* copy last 15 bytes, dest 4-bytes aligned */
-        "mov " LEN_REG ", " TMP_REG "\n\t"
-        "and $3, " LEN_REG "\n\t"
-        "shr $3, " TMP_REG "\n\t"
-        "jnc 1f\n\t"
-        "movsl\n\t"
-        "1:\n\t"
-        "shr $1, " TMP_REG "\n\t"
-        "jnc copy_fwd3\n\t"
-        "movsl\n\t"
-        "movsl\n\t"
-        "copy_fwd3:\n\t" /* copy last 3 bytes */
-        "shr $1, " LEN_REG "\n\t"
-        "jnc 1f\n\t"
-        "movsb\n\t"
-        "1:\n\t"
-        "shr $1, " LEN_REG "\n\t"
-        "jnc 1f\n\t"
-        "movsw\n\t"
-        "1:\n\t"
-        MEMMOVE_CLEANUP
-        "ret\n\t"
-        "copy_bwd:\n\t"
-        "lea (" DEST_REG ", " LEN_REG "), " DEST_REG "\n\t"
-        "lea (" SRC_REG ", " LEN_REG "), " SRC_REG "\n\t"
-        "cmp $4, " LEN_REG "\n\t" /* 4-bytes align */
-        "jb copy_bwd3\n\t"
-        "mov " DEST_REG ", " TMP_REG "\n\t"
-        "shr $1, " TMP_REG "\n\t"
-        "jnc 1f\n\t"
-        "dec " SRC_REG "\n\t"
-        "dec " DEST_REG "\n\t"
-        "movb (" SRC_REG "), %al\n\t"
-        "movb %al, (" DEST_REG ")\n\t"
-        "dec " LEN_REG "\n\t"
-        "1:\n\t"
-        "shr $1, " TMP_REG "\n\t"
-        "jnc 1f\n\t"
-        "sub $2, " SRC_REG "\n\t"
-        "sub $2, " DEST_REG "\n\t"
-        "movw (" SRC_REG "), %ax\n\t"
-        "movw %ax, (" DEST_REG ")\n\t"
-        "sub $2, " LEN_REG "\n\t"
-        "1:\n\t" /* 16-bytes align */
-        "cmp $16, " LEN_REG "\n\t"
-        "jb copy_bwd15\n\t"
-        "shr $1, " TMP_REG "\n\t"
-        "jnc 1f\n\t"
-        "sub $4, " SRC_REG "\n\t"
-        "sub $4, " DEST_REG "\n\t"
-        "movl (" SRC_REG "), %eax\n\t"
-        "movl %eax, (" DEST_REG ")\n\t"
-        "sub $4, " LEN_REG "\n\t"
-        "1:\n\t"
-        "shr $1, " TMP_REG "\n\t"
-        "jnc 1f\n\t"
-        "sub $8, " SRC_REG "\n\t"
-        "sub $8, " DEST_REG "\n\t"
-        "movl 4(" SRC_REG "), %eax\n\t"
-        "movl %eax, 4(" DEST_REG ")\n\t"
-        "movl (" SRC_REG "), %eax\n\t"
-        "movl %eax, (" DEST_REG ")\n\t"
-        "sub $8, " LEN_REG "\n\t"
-        "1:\n\t"
-        "cmp $64, " LEN_REG "\n\t"
-        "jb copy_bwd63\n\t"
-        "1:\n\t" /* copy 64-bytes blocks in loop, dest 16-bytes aligned */
-        "sub $64, " SRC_REG "\n\t"
-        "sub $64, " DEST_REG "\n\t"
-        "movdqu 0x00(" SRC_REG "), %xmm0\n\t"
-        "movdqu 0x10(" SRC_REG "), %xmm1\n\t"
-        "movdqu 0x20(" SRC_REG "), %xmm2\n\t"
-        "movdqu 0x30(" SRC_REG "), %xmm3\n\t"
-        "movdqa %xmm0, 0x00(" DEST_REG ")\n\t"
-        "movdqa %xmm1, 0x10(" DEST_REG ")\n\t"
-        "movdqa %xmm2, 0x20(" DEST_REG ")\n\t"
-        "movdqa %xmm3, 0x30(" DEST_REG ")\n\t"
-        "sub $64, " LEN_REG "\n\t"
-        "cmp $64, " LEN_REG "\n\t"
-        "jae 1b\n\t"
-        "copy_bwd63:\n\t" /* copy last 63 bytes, dest 16-bytes aligned */
-        "mov " LEN_REG ", " TMP_REG "\n\t"
-        "and $15, " LEN_REG "\n\t"
-        "shr $5, " TMP_REG "\n\t"
-        "jnc 1f\n\t"
-        "sub $16, " SRC_REG "\n\t"
-        "sub $16, " DEST_REG "\n\t"
-        "movdqu (" SRC_REG "), %xmm0\n\t"
-        "movdqa %xmm0, (" DEST_REG ")\n\t"
-        "1:\n\t"
-        "shr $1, " TMP_REG "\n\t"
-        "jnc copy_bwd15\n\t"
-        "sub $32, " SRC_REG "\n\t"
-        "sub $32, " DEST_REG "\n\t"
-        "movdqu 0x00(" SRC_REG "), %xmm0\n\t"
-        "movdqu 0x10(" SRC_REG "), %xmm1\n\t"
-        "movdqa %xmm0, 0x00(" DEST_REG ")\n\t"
-        "movdqa %xmm1, 0x10(" DEST_REG ")\n\t"
-        "copy_bwd15:\n\t" /* copy last 15 bytes, dest 4-bytes aligned */
-        "mov " LEN_REG ", " TMP_REG "\n\t"
-        "and $3, " LEN_REG "\n\t"
-        "shr $3, " TMP_REG "\n\t"
-        "jnc 1f\n\t"
-        "sub $4, " SRC_REG "\n\t"
-        "sub $4, " DEST_REG "\n\t"
-        "movl (" SRC_REG "), %eax\n\t"
-        "movl %eax, (" DEST_REG ")\n\t"
-        "1:\n\t"
-        "shr $1, " TMP_REG "\n\t"
-        "jnc copy_bwd3\n\t"
-        "sub $8, " SRC_REG "\n\t"
-        "sub $8, " DEST_REG "\n\t"
-        "movl 4(" SRC_REG "), %eax\n\t"
-        "movl %eax, 4(" DEST_REG ")\n\t"
-        "movl (" SRC_REG "), %eax\n\t"
-        "movl %eax, (" DEST_REG ")\n\t"
-        "copy_bwd3:\n\t" /* copy last 3 bytes */
-        "shr $1, " LEN_REG "\n\t"
-        "jnc 1f\n\t"
-        "dec " SRC_REG "\n\t"
-        "dec " DEST_REG "\n\t"
-        "movb (" SRC_REG "), %al\n\t"
-        "movb %al, (" DEST_REG ")\n\t"
-        "1:\n\t"
-        "shr $1, " LEN_REG "\n\t"
-        "jnc 1f\n\t"
-        "movw -2(" SRC_REG "), %ax\n\t"
-        "movw %ax, -2(" DEST_REG ")\n\t"
-        "1:\n\t"
-        MEMMOVE_CLEANUP
-        "ret" )
+#if !defined(__cpuidex) && (defined(__i386__) || (defined(__x86_64__) && !defined(__arm64ec__)))
 
+#if __has_builtin(__cpuidex) || (defined(_MSC_VER) && !defined(__clang__))
+void __cpuidex(int info[4], int ax, int cx);
+#pragma intrinsic(__cpuidex)
+#else
+static inline void __cpuidex(int info[4], int ax, int cx)
+{
+  __asm__ ("cpuid" : "=a"(info[0]), "=b" (info[1]), "=c"(info[2]), "=d"(info[3]) : "a"(ax), "c"(cx));
+}
 #endif
 
-/*********************************************************************
- *                  memmove (MSVCRT.@)
- */
-#ifdef WORDS_BIGENDIAN
-# define MERGE(w1, sh1, w2, sh2) ((w1 << sh1) | (w2 >> sh2))
+#if __has_builtin(__cpuid) || (defined(_MSC_VER) && !defined(__clang__))
+void __cpuid(int info[4], int ax);
+#pragma intrinsic(__cpuid)
 #else
-# define MERGE(w1, sh1, w2, sh2) ((w1 >> sh1) | (w2 << sh2))
-#endif
-void * __cdecl memmove(void *dst, const void *src, size_t n)
+static inline void __cpuid(int info[4], int ax)
 {
-#if defined(__x86_64__) && !defined(__arm64ec__)
-    return sse2_memmove(dst, src, n);
-#else
-    unsigned char *d = dst;
-    const unsigned char *s = src;
-    int sh1;
-
-#ifdef __i386__
-    if (sse2_supported)
-        return sse2_memmove(dst, src, n);
+    return __cpuidex(info, ax, 0);
+}
 #endif
 
-    if (!n) return dst;
+#endif
 
-    if ((size_t)dst - (size_t)src >= n)
+static inline int cpu_supports(const int featurelevel)
+{
+    static int cpu_featurelevel = -1;
+    if (unlikely(cpu_featurelevel < 0))
     {
-        for (; (size_t)d % sizeof(size_t) && n; n--) *d++ = *s++;
+        cpu_featurelevel = 0;
+        int regs[4];
+        int extended_regs[4];
 
-        sh1 = 8 * ((size_t)s % sizeof(size_t));
-        if (!sh1)
-        {
-            while (n >= sizeof(size_t))
-            {
-                *(size_t*)d = *(size_t*)s;
-                s += sizeof(size_t);
-                d += sizeof(size_t);
-                n -= sizeof(size_t);
-            }
-        }
-        else if (n >= 2 * sizeof(size_t))
-        {
-            int sh2 = 8 * sizeof(size_t) - sh1;
-            size_t x, y;
+        __cpuid(regs, 1);
+        __cpuidex(extended_regs, 7, 0);
 
-            s -= sh1 / 8;
-            x = *(size_t*)s;
-            do
-            {
-                s += sizeof(size_t);
-                y = *(size_t*)s;
-                *(size_t*)d = MERGE(x, sh1, y, sh2);
-                d += sizeof(size_t);
-
-                s += sizeof(size_t);
-                x = *(size_t*)s;
-                *(size_t*)d = MERGE(y, sh1, x, sh2);
-                d += sizeof(size_t);
-
-                n -= 2 * sizeof(size_t);
-            } while (n >= 2 * sizeof(size_t));
-            s += sh1 / 8;
-        }
-        while (n--) *d++ = *s++;
-        return dst;
+        const int edx_features = regs[3];
+        const int ecx_features = regs[2];
+        const int ebx_features = extended_regs[1];
+
+        cpu_featurelevel += !!(ecx_features & (1 << 20)) + (ecx_features & (1 << 28)) && (ebx_features & (1 << 5)) + !!(ebx_features & (1 << 16));
     }
-    else
-    {
-        d += n;
-        s += n;
+    return (cpu_featurelevel >= featurelevel);
+}
 
-        for (; (size_t)d % sizeof(size_t) && n; n--) *--d = *--s;
+#define FEAT_AVX512 3
+#define FEAT_AVX2 2
+#define FEAT_SSE42 1
 
-        sh1 = 8 * ((size_t)s % sizeof(size_t));
-        if (!sh1)
-        {
-            while (n >= sizeof(size_t))
-            {
-                s -= sizeof(size_t);
-                d -= sizeof(size_t);
-                *(size_t*)d = *(size_t*)s;
-                n -= sizeof(size_t);
-            }
-        }
-        else if (n >= 2 * sizeof(size_t))
-        {
-            int sh2 = 8 * sizeof(size_t) - sh1;
-            size_t x, y;
-
-            s -= sh1 / 8;
-            x = *(size_t*)s;
-            do
-            {
-                s -= sizeof(size_t);
-                y = *(size_t*)s;
-                d -= sizeof(size_t);
-                *(size_t*)d = MERGE(y, sh1, x, sh2);
-
-                s -= sizeof(size_t);
-                x = *(size_t*)s;
-                d -= sizeof(size_t);
-                *(size_t*)d = MERGE(x, sh1, y, sh2);
-
-                n -= 2 * sizeof(size_t);
-            } while (n >= 2 * sizeof(size_t));
-            s += sh1 / 8;
-        }
-        while (n--) *--d = *--s;
+#define IMPLEMENT_MEMMOVE(arch) \
+    NOBUILTIN static void* memmove_##arch(void* dst, const void* src, size_t n) \
+    { \
+        unsigned char *d = dst; \
+        const unsigned char *s = src; \
+        if (d == s || !n) \
+            return dst; \
+        if ((size_t)dst - (size_t)src >= n) \
+        { \
+            while (n--) *d++ = *s++; \
+        } \
+        else \
+        { \
+            d += n - 1; \
+            s += n - 1; \
+            while (n--) *d-- = *s--; \
+        } \
+        return dst; \
     }
-    return dst;
+
+#ifndef __AVX512F__
+#ifdef __clang__
+#pragma clang attribute push(__attribute__((target("avx512f,arch=x86-64-v4,tune=znver4"))), apply_to = function)
+#else
+#pragma GCC push_options
+#pragma GCC target("avx512f,arch=x86-64-v4,tune=znver4")
+#endif
+#define has_avx512f cpu_supports(FEAT_AVX512)
+#else
+#define has_avx512f 1
+#endif
+
+IMPLEMENT_MEMMOVE(avx512f)
+
+#ifndef __AVX512F__
+#ifdef __clang__
+#pragma clang attribute pop
+#else
+#pragma GCC pop_options
 #endif
+#endif
+
+#ifndef __AVX2__
+#ifdef __clang__
+#pragma clang attribute push(__attribute__((target("avx2,arch=x86-64-v3,tune=haswell"))), apply_to = function)
+#else
+#pragma GCC push_options
+#pragma GCC target("avx2,arch=x86-64-v3,tune=haswell")
+#endif
+#define has_avx2 cpu_supports(FEAT_AVX2)
+#else
+#define has_avx2 1
+#endif
+
+IMPLEMENT_MEMMOVE(avx2)
+
+#ifndef __AVX2__
+#ifdef __clang__
+#pragma clang attribute pop
+#else
+#pragma GCC pop_options
+#endif
+#endif
+
+#ifndef __SSE4_2__
+#ifdef __clang__
+#pragma clang attribute push(__attribute__((target("sse4.2,arch=x86-64-v2,tune=nehalem"))), apply_to = function)
+#else
+#pragma GCC push_options
+#pragma GCC target("sse4.2,arch=x86-64-v2,tune=nehalem")
+#endif
+#define has_sse42 cpu_supports(FEAT_SSE42)
+#else
+#define has_sse42 1
+#endif
+
+IMPLEMENT_MEMMOVE(sse42)
+
+#ifndef __SSE4_2__
+#ifdef __clang__
+#pragma clang attribute pop
+#else
+#pragma GCC pop_options
+#endif
+#endif
+
+IMPLEMENT_MEMMOVE(base)
+
+/*********************************************************************
+ *                  memmove (MSVCRT.@)
+ */
+void * __cdecl memmove(void *dst, const void *src, size_t n)
+{
+    if (has_avx512f)
+        return memmove_avx512f(dst, src, n);
+    if (likely(has_avx2))
+        return memmove_avx2(dst, src, n);
+    if (likely(has_sse42))
+        return memmove_sse42(dst, src, n);
+
+    return memmove_base(dst, src, n);
 }
-#undef MERGE
 
 /*********************************************************************
  *                  memcpy   (MSVCRT.@)
@@ -3174,74 +2970,86 @@ void * __cdecl _memccpy(void *dst, const void *src, int c, size_t n)
     return NULL;
 }
 
-
-static inline void memset_aligned_32(unsigned char *d, uint64_t v, size_t n)
-{
-    unsigned char *end = d + n;
-    while (d < end)
-    {
-        *(uint64_t *)(d + 0) = v;
-        *(uint64_t *)(d + 8) = v;
-        *(uint64_t *)(d + 16) = v;
-        *(uint64_t *)(d + 24) = v;
-        d += 32;
+#define IMPLEMENT_MEMSET(arch) \
+    NOBUILTIN static void* memset_##arch(void* dst, int c, size_t n) \
+    { \
+        unsigned char *d = dst; \
+        while (n--) *d++ = c; \
+        return dst; \
     }
-}
+
+#ifndef __AVX512F__
+#ifdef __clang__
+#pragma clang attribute push(__attribute__((target("avx512f,arch=x86-64-v4,tune=znver4"))), apply_to = function)
+#else
+#pragma GCC push_options
+#pragma GCC target("avx512f,arch=x86-64-v4,tune=znver4")
+#endif
+#endif
+
+IMPLEMENT_MEMSET(avx512f)
+
+#ifndef __AVX512F__
+#ifdef __clang__
+#pragma clang attribute pop
+#else
+#pragma GCC pop_options
+#endif
+#endif
+
+#ifndef __AVX2__
+#ifdef __clang__
+#pragma clang attribute push(__attribute__((target("avx2,arch=x86-64-v3,tune=haswell"))), apply_to = function)
+#else
+#pragma GCC push_options
+#pragma GCC target("avx2,arch=x86-64-v3,tune=haswell")
+#endif
+#endif
+
+IMPLEMENT_MEMSET(avx2)
+
+#ifndef __AVX2__
+#ifdef __clang__
+#pragma clang attribute pop
+#else
+#pragma GCC pop_options
+#endif
+#endif
+
+#ifndef __SSE4_2__
+#ifdef __clang__
+#pragma clang attribute push(__attribute__((target("sse4.2,arch=x86-64-v2,tune=nehalem"))), apply_to = function)
+#else
+#pragma GCC push_options
+#pragma GCC target("sse4.2,arch=x86-64-v2,tune=nehalem")
+#endif
+#endif
+
+IMPLEMENT_MEMSET(sse42)
+
+#ifndef __SSE4_2__
+#ifdef __clang__
+#pragma clang attribute pop
+#else
+#pragma GCC pop_options
+#endif
+#endif
+
+IMPLEMENT_MEMSET(base)
 
 /*********************************************************************
  *		    memset (MSVCRT.@)
  */
 void *__cdecl memset(void *dst, int c, size_t n)
 {
-    typedef uint64_t DECLSPEC_ALIGN(1) unaligned_ui64;
-    typedef uint32_t DECLSPEC_ALIGN(1) unaligned_ui32;
-    typedef uint16_t DECLSPEC_ALIGN(1) unaligned_ui16;
-
-    uint64_t v = 0x101010101010101ull * (unsigned char)c;
-    unsigned char *d = (unsigned char *)dst;
-    size_t a = 0x20 - ((uintptr_t)d & 0x1f);
-
-    if (n >= 16)
-    {
-        *(unaligned_ui64 *)(d + 0) = v;
-        *(unaligned_ui64 *)(d + 8) = v;
-        *(unaligned_ui64 *)(d + n - 16) = v;
-        *(unaligned_ui64 *)(d + n - 8) = v;
-        if (n <= 32) return dst;
-        *(unaligned_ui64 *)(d + 16) = v;
-        *(unaligned_ui64 *)(d + 24) = v;
-        *(unaligned_ui64 *)(d + n - 32) = v;
-        *(unaligned_ui64 *)(d + n - 24) = v;
-        if (n <= 64) return dst;
+    if (has_avx512f)
+        return memset_avx512f(dst, c, n);
+    if (likely(has_avx2))
+        return memset_avx2(dst, c, n);
+    if (likely(has_sse42))
+        return memset_sse42(dst, c, n);
 
-        n = (n - a) & ~0x1f;
-        memset_aligned_32(d + a, v, n);
-        return dst;
-    }
-    if (n >= 8)
-    {
-        *(unaligned_ui64 *)d = v;
-        *(unaligned_ui64 *)(d + n - 8) = v;
-        return dst;
-    }
-    if (n >= 4)
-    {
-        *(unaligned_ui32 *)d = v;
-        *(unaligned_ui32 *)(d + n - 4) = v;
-        return dst;
-    }
-    if (n >= 2)
-    {
-        *(unaligned_ui16 *)d = v;
-        *(unaligned_ui16 *)(d + n - 2) = v;
-        return dst;
-    }
-    if (n >= 1)
-    {
-        *(uint8_t *)d = v;
-        return dst;
-    }
-    return dst;
+    return memset_base(dst, c, n);
 }
 
 /*********************************************************************
diff --git a/dlls/ntdll/string.c b/dlls/ntdll/string.c
index 46fb80a3a69..61ee468495b 100644
--- a/dlls/ntdll/string.c
+++ b/dlls/ntdll/string.c
@@ -94,51 +94,178 @@ int __cdecl memcmp( const void *ptr1, const void *ptr2, size_t n )
     return 0;
 }
 
+#ifndef likely
+#define likely(x) __builtin_expect(!!(x), 1)
+#endif
+#ifndef unlikely
+#define unlikely(x) __builtin_expect(!!(x), 0)
+#endif
 
-/*********************************************************************
- *                  memcpy   (NTDLL.@)
- *
- * NOTES
- *  Behaves like memmove.
- */
-void * __cdecl memcpy( void *dst, const void *src, size_t n )
+#ifdef __clang__
+#define NOBUILTIN __attribute__((no_builtin("memcpy", "memmove", "memset", "memcmp")))
+#else
+#define NOBUILTIN __attribute__((__optimize__("-fno-tree-loop-distribute-patterns")))
+#endif
+
+#if !defined(__cpuidex) && (defined(__i386__) || (defined(__x86_64__) && !defined(__arm64ec__)))
+
+#if __has_builtin(__cpuidex) || (defined(_MSC_VER) && !defined(__clang__))
+void __cpuidex(int info[4], int ax, int cx);
+#pragma intrinsic(__cpuidex)
+#else
+static inline void __cpuidex(int info[4], int ax, int cx)
 {
-    volatile unsigned char *d = dst;  /* avoid gcc optimizations */
-    const unsigned char *s = src;
+  __asm__ ("cpuid" : "=a"(info[0]), "=b" (info[1]), "=c"(info[2]), "=d"(info[3]) : "a"(ax), "c"(cx));
+}
+#endif
+
+#if __has_builtin(__cpuid) || (defined(_MSC_VER) && !defined(__clang__))
+void __cpuid(int info[4], int ax);
+#pragma intrinsic(__cpuid)
+#else
+static inline void __cpuid(int info[4], int ax)
+{
+    return __cpuidex(info, ax, 0);
+}
+#endif
+
+#endif
 
-    if ((size_t)dst - (size_t)src >= n)
+static inline int cpu_supports(const int featurelevel)
+{
+    static int cpu_featurelevel = -1;
+    if (unlikely(cpu_featurelevel < 0))
     {
-        while (n--) *d++ = *s++;
+        cpu_featurelevel = 0;
+
+        int regs[4];
+        int extended_regs[4];
+
+        __cpuid(regs, 1);
+        __cpuidex(extended_regs, 7, 0);
+
+        const int edx_features = regs[3];
+        const int ecx_features = regs[2];
+        const int ebx_features = extended_regs[1];
+
+        cpu_featurelevel += !!(ecx_features & (1 << 20)) + (ecx_features & (1 << 28)) && (ebx_features & (1 << 5)) + !!(ebx_features & (1 << 16));
     }
-    else
-    {
-        d += n - 1;
-        s += n - 1;
-        while (n--) *d-- = *s--;
+    return (cpu_featurelevel >= featurelevel);
+}
+
+#define FEAT_AVX512 3
+#define FEAT_AVX2 2
+#define FEAT_SSE42 1
+
+#define IMPLEMENT_MEMMOVE(arch) \
+    NOBUILTIN static void* memmove_##arch(void* dst, const void* src, size_t n) \
+    { \
+        unsigned char *d = dst; \
+        const unsigned char *s = src; \
+        if (d == s || !n) \
+            return dst; \
+        if ((size_t)dst - (size_t)src >= n) \
+        { \
+            while (n--) *d++ = *s++; \
+        } \
+        else \
+        { \
+            d += n - 1; \
+            s += n - 1; \
+            while (n--) *d-- = *s--; \
+        } \
+        return dst; \
     }
-    return dst;
-}
 
+#ifndef __AVX512F__
+#ifdef __clang__
+#pragma clang attribute push(__attribute__((target("avx512f,arch=x86-64-v4,tune=znver4"))), apply_to = function)
+#else
+#pragma GCC push_options
+#pragma GCC target("avx512f,arch=x86-64-v4,tune=znver4")
+#endif
+#define has_avx512f cpu_supports(FEAT_AVX512)
+#else
+#define has_avx512f 1
+#endif
+
+IMPLEMENT_MEMMOVE(avx512f)
+
+#ifndef __AVX512F__
+#ifdef __clang__
+#pragma clang attribute pop
+#else
+#pragma GCC pop_options
+#endif
+#endif
+
+#ifndef __AVX2__
+#ifdef __clang__
+#pragma clang attribute push(__attribute__((target("avx2,arch=x86-64-v3,tune=haswell"))), apply_to = function)
+#else
+#pragma GCC push_options
+#pragma GCC target("avx2,arch=x86-64-v3,tune=haswell")
+#endif
+#define has_avx2 cpu_supports(FEAT_AVX2)
+#else
+#define has_avx2 1
+#endif
+
+IMPLEMENT_MEMMOVE(avx2)
+
+#ifndef __AVX2__
+#ifdef __clang__
+#pragma clang attribute pop
+#else
+#pragma GCC pop_options
+#endif
+#endif
+
+#ifndef __SSE4_2__
+#ifdef __clang__
+#pragma clang attribute push(__attribute__((target("sse4.2,arch=x86-64-v2,tune=nehalem"))), apply_to = function)
+#else
+#pragma GCC push_options
+#pragma GCC target("sse4.2,arch=x86-64-v2,tune=nehalem")
+#endif
+#define has_sse42 cpu_supports(FEAT_SSE42)
+#else
+#define has_sse42 1
+#endif
+
+IMPLEMENT_MEMMOVE(sse42)
+
+#ifndef __SSE4_2__
+#ifdef __clang__
+#pragma clang attribute pop
+#else
+#pragma GCC pop_options
+#endif
+#endif
+
+IMPLEMENT_MEMMOVE(base)
 
 /*********************************************************************
  *                  memmove   (NTDLL.@)
  */
 void * __cdecl memmove( void *dst, const void *src, size_t n )
 {
-    volatile unsigned char *d = dst;  /* avoid gcc optimizations */
-    const unsigned char *s = src;
+    if (has_avx512f)
+        return memmove_avx512f(dst, src, n);
+    if (likely(has_avx2))
+        return memmove_avx2(dst, src, n);
+    if (likely(has_sse42))
+        return memmove_sse42(dst, src, n);
 
-    if ((size_t)dst - (size_t)src >= n)
-    {
-        while (n--) *d++ = *s++;
-    }
-    else
-    {
-        d += n - 1;
-        s += n - 1;
-        while (n--) *d-- = *s--;
-    }
-    return dst;
+    return memmove_base(dst, src, n);
+}
+
+/*********************************************************************
+ *                  memcpy   (NTDLL.@)
+ */
+void * __cdecl memcpy( void *dst, const void *src, size_t n )
+{
+    return memmove( dst, src, n );
 }
 
 
@@ -177,74 +304,86 @@ errno_t __cdecl memmove_s( void *dst, size_t len, const void *src, size_t count
     return 0;
 }
 
-
-static inline void memset_aligned_32( unsigned char *d, uint64_t v, size_t n )
-{
-    unsigned char *end = d + n;
-    while (d < end)
-    {
-        *(uint64_t *)(d + 0) = v;
-        *(uint64_t *)(d + 8) = v;
-        *(uint64_t *)(d + 16) = v;
-        *(uint64_t *)(d + 24) = v;
-        d += 32;
+#define IMPLEMENT_MEMSET(arch) \
+    NOBUILTIN static void* memset_##arch(void* dst, int c, size_t n) \
+    { \
+        unsigned char *d = dst; \
+        while (n--) *d++ = c; \
+        return dst; \
     }
-}
+
+#ifndef __AVX512F__
+#ifdef __clang__
+#pragma clang attribute push(__attribute__((target("avx512f,arch=x86-64-v4,tune=znver4"))), apply_to = function)
+#else
+#pragma GCC push_options
+#pragma GCC target("avx512f,arch=x86-64-v4,tune=znver4")
+#endif
+#endif
+
+IMPLEMENT_MEMSET(avx512f)
+
+#ifndef __AVX512F__
+#ifdef __clang__
+#pragma clang attribute pop
+#else
+#pragma GCC pop_options
+#endif
+#endif
+
+#ifndef __AVX2__
+#ifdef __clang__
+#pragma clang attribute push(__attribute__((target("avx2,arch=x86-64-v3,tune=haswell"))), apply_to = function)
+#else
+#pragma GCC push_options
+#pragma GCC target("avx2,arch=x86-64-v3,tune=haswell")
+#endif
+#endif
+
+IMPLEMENT_MEMSET(avx2)
+
+#ifndef __AVX2__
+#ifdef __clang__
+#pragma clang attribute pop
+#else
+#pragma GCC pop_options
+#endif
+#endif
+
+#ifndef __SSE4_2__
+#ifdef __clang__
+#pragma clang attribute push(__attribute__((target("sse4.2,arch=x86-64-v2,tune=nehalem"))), apply_to = function)
+#else
+#pragma GCC push_options
+#pragma GCC target("sse4.2,arch=x86-64-v2,tune=nehalem")
+#endif
+#endif
+
+IMPLEMENT_MEMSET(sse42)
+
+#ifndef __SSE4_2__
+#ifdef __clang__
+#pragma clang attribute pop
+#else
+#pragma GCC pop_options
+#endif
+#endif
+
+IMPLEMENT_MEMSET(base)
 
 /*********************************************************************
  *                  memset   (NTDLL.@)
  */
 void *__cdecl memset( void *dst, int c, size_t n )
 {
-    typedef uint64_t DECLSPEC_ALIGN(1) unaligned_ui64;
-    typedef uint32_t DECLSPEC_ALIGN(1) unaligned_ui32;
-    typedef uint16_t DECLSPEC_ALIGN(1) unaligned_ui16;
-
-    uint64_t v = 0x101010101010101ull * (unsigned char)c;
-    unsigned char *d = (unsigned char *)dst;
-    size_t a = 0x20 - ((uintptr_t)d & 0x1f);
+    if (has_avx512f)
+        return memset_avx512f(dst, c, n);
+    if (likely(has_avx2))
+        return memset_avx2(dst, c, n);
+    if (likely(has_sse42))
+        return memset_sse42(dst, c, n);
 
-    if (n >= 16)
-    {
-        *(unaligned_ui64 *)(d + 0) = v;
-        *(unaligned_ui64 *)(d + 8) = v;
-        *(unaligned_ui64 *)(d + n - 16) = v;
-        *(unaligned_ui64 *)(d + n - 8) = v;
-        if (n <= 32) return dst;
-        *(unaligned_ui64 *)(d + 16) = v;
-        *(unaligned_ui64 *)(d + 24) = v;
-        *(unaligned_ui64 *)(d + n - 32) = v;
-        *(unaligned_ui64 *)(d + n - 24) = v;
-        if (n <= 64) return dst;
-
-        n = (n - a) & ~0x1f;
-        memset_aligned_32( d + a, v, n );
-        return dst;
-    }
-    if (n >= 8)
-    {
-        *(unaligned_ui64 *)d = v;
-        *(unaligned_ui64 *)(d + n - 8) = v;
-        return dst;
-    }
-    if (n >= 4)
-    {
-        *(unaligned_ui32 *)d = v;
-        *(unaligned_ui32 *)(d + n - 4) = v;
-        return dst;
-    }
-    if (n >= 2)
-    {
-        *(unaligned_ui16 *)d = v;
-        *(unaligned_ui16 *)(d + n - 2) = v;
-        return dst;
-    }
-    if (n >= 1)
-    {
-        *(uint8_t *)d = v;
-        return dst;
-    }
-    return dst;
+    return memset_base(dst, c, n);
 }
 
 
-- 
2.48.1

