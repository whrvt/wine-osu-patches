From 5180717ae134b61bf80fd49dffa1853d9cace0d3 Mon Sep 17 00:00:00 2001
From: Elizabeth Figura <zfigura@codeweavers.com>
Date: Fri, 12 Mar 2021 23:04:17 +0200
Subject: [PATCH 27/32] ntdll: Cache in-process synchronization objects.

---
 dlls/ntdll/unix/server.c       |   9 ++
 dlls/ntdll/unix/sync.c         | 195 +++++++++++++++++++++++++++++++--
 dlls/ntdll/unix/unix_private.h |   4 +
 3 files changed, 197 insertions(+), 11 deletions(-)

diff --git a/dlls/ntdll/unix/server.c b/dlls/ntdll/unix/server.c
index d31d31be64f..b4e7902cf1b 100644
--- a/dlls/ntdll/unix/server.c
+++ b/dlls/ntdll/unix/server.c
@@ -1807,12 +1807,17 @@ NTSTATUS WINAPI NtDuplicateObject( HANDLE source_process, HANDLE source, HANDLE
         return result.dup_handle.status;
     }
 
+    /* hold fd_cache_mutex to prevent the fd from being added again between the
+     * call to remove_fd_from_cache and close_handle */
     server_enter_uninterrupted_section( &fd_cache_mutex, &sigset );
 
     /* always remove the cached fd; if the server request fails we'll just
      * retrieve it again */
     if (options & DUPLICATE_CLOSE_SOURCE)
+    {
         fd = remove_fd_from_cache( source );
+        close_inproc_sync_obj( source );
+    }
 
     SERVER_START_REQ( dup_handle )
     {
@@ -1868,6 +1873,8 @@ NTSTATUS WINAPI NtClose( HANDLE handle )
     if (HandleToLong( handle ) >= ~5 && HandleToLong( handle ) <= ~0)
         return STATUS_SUCCESS;
 
+    /* hold fd_cache_mutex to prevent the fd from being added again between the
+     * call to remove_fd_from_cache and close_handle */
     server_enter_uninterrupted_section( &fd_cache_mutex, &sigset );
 
     /* always remove the cached fd; if the server request fails we'll just
@@ -1880,6 +1887,8 @@ NTSTATUS WINAPI NtClose( HANDLE handle )
     if (do_esync())
         esync_close( handle );
 
+    close_inproc_sync_obj( handle );
+
     SERVER_START_REQ( close_handle )
     {
         req->handle = wine_server_obj_handle( handle );
diff --git a/dlls/ntdll/unix/sync.c b/dlls/ntdll/unix/sync.c
index 79a11c84203..bb141da7610 100644
--- a/dlls/ntdll/unix/sync.c
+++ b/dlls/ntdll/unix/sync.c
@@ -325,6 +325,12 @@ static int get_linux_sync_device(void)
  * it closes the handle; when all handles are closed, the server deletes the
  * in-process synchronization object.
  *
+ * We also need this for signal-and-wait. The signal and wait operations aren't
+ * atomic, but we can't perform the signal and then return STATUS_INVALID_HANDLE
+ * for the waitâ€”we need to either do both operations or neither. That means we
+ * need to grab references to both objects, and prevent them from being
+ * destroyed before we're done with them.
+ *
  * We want lookup of objects from the cache to be very fast; ideally, it should
  * be lock-free. We achieve this by using atomic modifications to "refcount",
  * and guaranteeing that all other fields are valid and correct *as long as*
@@ -368,13 +374,140 @@ static void release_inproc_sync_obj( struct inproc_sync_cache_entry *cache )
 
     if (!refcount)
     {
-        NTSTATUS ret = NtClose( handle );
+        NTSTATUS ret;
+
+        /* we can't call NtClose here as we may be inside fd_cache_mutex */
+        SERVER_START_REQ( close_handle )
+        {
+            req->handle = wine_server_obj_handle( handle );
+            ret = wine_server_call( req );
+        }
+        SERVER_END_REQ;
+
         assert( !ret );
         close( fd );
     }
 }
 
 
+#define INPROC_SYNC_CACHE_BLOCK_SIZE  (65536 / sizeof(struct inproc_sync_cache_entry))
+#define INPROC_SYNC_CACHE_ENTRIES     128
+
+static struct inproc_sync_cache_entry *inproc_sync_cache[INPROC_SYNC_CACHE_ENTRIES];
+static struct inproc_sync_cache_entry inproc_sync_cache_initial_block[INPROC_SYNC_CACHE_BLOCK_SIZE];
+
+static inline unsigned int inproc_sync_handle_to_index( HANDLE handle, unsigned int *entry )
+{
+    unsigned int idx = (wine_server_obj_handle(handle) >> 2) - 1;
+    *entry = idx / INPROC_SYNC_CACHE_BLOCK_SIZE;
+    return idx % INPROC_SYNC_CACHE_BLOCK_SIZE;
+}
+
+
+static struct inproc_sync_cache_entry *cache_inproc_sync_obj( HANDLE handle, obj_handle_t inproc_sync, int fd,
+                                                              enum inproc_sync_type type, unsigned int access )
+{
+    unsigned int entry, idx = inproc_sync_handle_to_index( handle, &entry );
+    struct inproc_sync_cache_entry *cache;
+    sigset_t sigset;
+    int refcount;
+
+    if (entry >= INPROC_SYNC_CACHE_ENTRIES)
+    {
+        FIXME( "too many allocated handles, not caching %p\n", handle );
+        return NULL;
+    }
+
+    if (!inproc_sync_cache[entry])  /* do we need to allocate a new block of entries? */
+    {
+        if (!entry) inproc_sync_cache[0] = inproc_sync_cache_initial_block;
+        else
+        {
+            static const size_t size = INPROC_SYNC_CACHE_BLOCK_SIZE * sizeof(struct inproc_sync_cache_entry);
+            void *ptr = anon_mmap_alloc( size, PROT_READ | PROT_WRITE );
+            if (ptr == MAP_FAILED) return NULL;
+            if (InterlockedCompareExchangePointer( (void **)&inproc_sync_cache[entry], ptr, NULL ))
+                munmap( ptr, size ); /* someone beat us to it */
+        }
+    }
+
+    cache = &inproc_sync_cache[entry][idx];
+
+    /* Hold fd_cache_mutex instead of a separate mutex, to prevent the same
+     * race between this function and NtClose. That is, prevent the object from
+     * being cached again between close_inproc_sync_obj() and close_handle. */
+    server_enter_uninterrupted_section( &fd_cache_mutex, &sigset );
+
+    if (InterlockedCompareExchange( &cache->refcount, 0, 0 ))
+    {
+        /* We lost the race with another thread trying to cache this object, or
+         * the handle is currently being used for another object (i.e. it was
+         * closed and then reused). We have no way of knowing which, and in the
+         * latter case we can't cache this object until the old one is
+         * completely destroyed, so always return failure. */
+        server_leave_uninterrupted_section( &fd_cache_mutex, &sigset );
+        return NULL;
+    }
+
+    cache->handle = inproc_sync;
+    cache->fd = fd;
+    cache->type = type;
+    cache->access = access;
+    cache->closed = FALSE;
+    /* Make sure we set the other members before the refcount; this store needs
+     * release semantics [paired with the load in get_cached_inproc_sync_obj()].
+     * Set the refcount to 2 (one for the handle, one for the caller). */
+    refcount = InterlockedExchange( &cache->refcount, 2 );
+    assert( !refcount );
+
+    server_leave_uninterrupted_section( &fd_cache_mutex, &sigset );
+
+    return cache;
+}
+
+
+/* returns the previous value */
+static inline LONG interlocked_inc_if_nonzero( LONG *dest )
+{
+    LONG val, tmp;
+    for (val = *dest;; val = tmp)
+    {
+        if (!val || (tmp = InterlockedCompareExchange( dest, val + 1, val )) == val)
+            break;
+    }
+    return val;
+}
+
+
+static struct inproc_sync_cache_entry *get_cached_inproc_sync_obj( HANDLE handle )
+{
+    unsigned int entry, idx = inproc_sync_handle_to_index( handle, &entry );
+    struct inproc_sync_cache_entry *cache;
+
+    if (entry >= INPROC_SYNC_CACHE_ENTRIES || !inproc_sync_cache[entry])
+        return NULL;
+
+    cache = &inproc_sync_cache[entry][idx];
+
+    /* this load needs acquire semantics [paired with the store in
+     * cache_inproc_sync_obj()] */
+    if (!interlocked_inc_if_nonzero( &cache->refcount ))
+        return NULL;
+
+    if (cache->closed)
+    {
+        /* The object is still being used, but "handle" has been closed. The
+         * handle value might have been reused for another object in the
+         * meantime, in which case we have to report that valid object, so
+         * force the caller to check the server. */
+        release_inproc_sync_obj( cache );
+        return NULL;
+    }
+
+    return cache;
+}
+
+
 static BOOL inproc_sync_types_match( enum inproc_sync_type a, enum inproc_sync_type b )
 {
     if (a == b) return TRUE;
@@ -391,32 +524,53 @@ static NTSTATUS get_inproc_sync_obj( HANDLE handle, enum inproc_sync_type desire
                                      struct inproc_sync_cache_entry *stack_cache,
                                      struct inproc_sync_cache_entry **ret_cache )
 {
-    struct inproc_sync_cache_entry *cache = stack_cache;
-    int needs_close;
+    struct inproc_sync_cache_entry *cache;
+    obj_handle_t inproc_sync_handle;
+    enum inproc_sync_type type;
+    unsigned int access;
+    int fd, needs_close;
     NTSTATUS ret;
 
-    *ret_cache = stack_cache;
+    /* try to find it in the cache already */
+    if ((cache = get_cached_inproc_sync_obj( handle )))
+    {
+        *ret_cache = cache;
+        return STATUS_SUCCESS;
+    }
 
+    /* try to retrieve it from the server */
     SERVER_START_REQ( get_linux_sync_obj )
     {
         req->handle = wine_server_obj_handle( handle );
         if (!(ret = wine_server_call( req )))
         {
-            cache->handle = reply->handle;
-            cache->access = reply->access;
-            cache->type = reply->type;
-            cache->refcount = 1;
-            cache->closed = FALSE;
+            inproc_sync_handle = reply->handle;
+            access = reply->access;
+            type = reply->type;
         }
     }
     SERVER_END_REQ;
 
     if (ret) return ret;
 
-    if ((ret = server_get_unix_fd( wine_server_ptr_handle( cache->handle ),
-                                   0, &cache->fd, &needs_close, NULL, NULL )))
+    if ((ret = server_get_unix_fd( wine_server_ptr_handle( inproc_sync_handle ),
+                                   0, &fd, &needs_close, NULL, NULL )))
         return ret;
 
+    cache = cache_inproc_sync_obj( handle, inproc_sync_handle, fd, type, access );
+    if (!cache)
+    {
+        cache = stack_cache;
+        cache->handle = inproc_sync_handle;
+        cache->fd = fd;
+        cache->type = type;
+        cache->access = access;
+        cache->closed = FALSE;
+        cache->refcount = 1;
+    }
+
+    *ret_cache = cache;
+
     if (desired_type && !inproc_sync_types_match( cache->type, desired_type ))
     {
         release_inproc_sync_obj( cache );
@@ -433,6 +587,21 @@ static NTSTATUS get_inproc_sync_obj( HANDLE handle, enum inproc_sync_type desire
 }
 
 
+/* caller must hold fd_cache_mutex */
+void close_inproc_sync_obj( HANDLE handle )
+{
+    struct inproc_sync_cache_entry *cache = get_cached_inproc_sync_obj( handle );
+
+    if (cache)
+    {
+        cache->closed = TRUE;
+        /* once for the reference we just grabbed, and once for the handle */
+        release_inproc_sync_obj( cache );
+        release_inproc_sync_obj( cache );
+    }
+}
+
+
 static NTSTATUS linux_release_semaphore_obj( int obj, ULONG count, ULONG *prev_count )
 {
     NTSTATUS ret;
@@ -938,6 +1107,10 @@ static NTSTATUS inproc_signal_and_wait( HANDLE signal, HANDLE wait,
 
 #else
 
+void close_inproc_sync_obj( HANDLE handle )
+{
+}
+
 static NTSTATUS inproc_release_semaphore( HANDLE handle, ULONG count, ULONG *prev_count )
 {
     return STATUS_NOT_IMPLEMENTED;
diff --git a/dlls/ntdll/unix/unix_private.h b/dlls/ntdll/unix/unix_private.h
index 02988a868f2..be6ee7be3e1 100644
--- a/dlls/ntdll/unix/unix_private.h
+++ b/dlls/ntdll/unix/unix_private.h
@@ -208,6 +208,8 @@ extern NTSTATUS load_main_exe( const WCHAR *name, const char *unix_name, const W
 extern NTSTATUS load_start_exe( WCHAR **image, void **module );
 extern void start_server( BOOL debug );
 
+extern pthread_mutex_t fd_cache_mutex;
+
 extern unsigned int server_call_unlocked( void *req_ptr );
 extern void server_enter_uninterrupted_section( pthread_mutex_t *mutex, sigset_t *sigset );
 extern void server_leave_uninterrupted_section( pthread_mutex_t *mutex, sigset_t *sigset );
@@ -383,6 +385,8 @@ extern NTSTATUS wow64_wine_spawnvp( void *args );
 
 extern void dbg_init(void);
 
+extern void close_inproc_sync_obj( HANDLE handle );
+
 extern NTSTATUS call_user_apc_dispatcher( CONTEXT *context_ptr, ULONG_PTR arg1, ULONG_PTR arg2, ULONG_PTR arg3,
                                           PNTAPCFUNC func, NTSTATUS status );
 extern NTSTATUS call_user_exception_dispatcher( EXCEPTION_RECORD *rec, CONTEXT *context );
-- 
2.47.1


