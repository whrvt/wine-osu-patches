From 61d0927a98936d2fde8eed8621c2f2fc65d9c816 Mon Sep 17 00:00:00 2001
From: Elizabeth Figura <zfigura@codeweavers.com>
Date: Tue, 6 Apr 2021 23:37:02 +0300
Subject: [PATCH 25/32] ntdll: Use in-process synchronization objects.

---
 dlls/ntdll/unix/sync.c         | 760 +++++++++++++++++++++++++++++++++
 dlls/ntdll/unix/unix_private.h |   2 +
 dlls/ntdll/unix/virtual.c      |   1 +
 3 files changed, 763 insertions(+)

diff --git a/dlls/ntdll/unix/sync.c b/dlls/ntdll/unix/sync.c
index 22563eb9930..a433ba456e4 100644
--- a/dlls/ntdll/unix/sync.c
+++ b/dlls/ntdll/unix/sync.c
@@ -30,9 +30,11 @@
 #include <assert.h>
 #include <errno.h>
 #include <fcntl.h>
+#include <inttypes.h>
 #include <limits.h>
 #include <signal.h>
 #include <sys/types.h>
+#include <sys/ioctl.h>
 #include <sys/mman.h>
 #ifdef HAVE_SYS_SYSCALL_H
 #include <sys/syscall.h>
@@ -48,6 +50,7 @@
 #endif
 #include <string.h>
 #include <stdarg.h>
+#include <stdint.h>
 #include <stdio.h>
 #include <stdlib.h>
 #include <time.h>
@@ -57,6 +60,9 @@
 #ifdef HAVE_KQUEUE
 # include <sys/event.h>
 #endif
+#ifdef HAVE_LINUX_NTSYNC_H
+# include <linux/ntsync.h>
+#endif
 
 #include "ntstatus.h"
 #define WIN32_NO_STATUS
@@ -265,6 +271,717 @@ static unsigned int validate_open_object_attributes( const OBJECT_ATTRIBUTES *at
 }
 
 
+#ifdef HAVE_LINUX_NTSYNC_H
+
+static int get_linux_sync_device(void)
+{
+    static LONG device = -2;
+
+    if (device == -2)
+    {
+        HANDLE handle;
+        int fd, needs_close;
+        NTSTATUS ret;
+
+        SERVER_START_REQ( get_linux_sync_device )
+        {
+            if (!(ret = wine_server_call( req ))) handle = wine_server_ptr_handle( reply->handle );
+        }
+        SERVER_END_REQ;
+
+        if (!ret)
+        {
+            if (!server_get_unix_fd( handle, 0, &fd, &needs_close, NULL, NULL ))
+            {
+                if (InterlockedCompareExchange( &device, fd, -2 ) != -2)
+                {
+                    /* someone beat us to it */
+                    if (needs_close) close( fd );
+                    NtClose( handle );
+                }
+                /* otherwise don't close the device */
+            }
+            else
+            {
+                InterlockedCompareExchange( &device, -1, -2 );
+                NtClose( handle );
+            }
+        }
+        else
+        {
+            InterlockedCompareExchange( &device, -1, -2 );
+        }
+    }
+    return device;
+}
+
+/* It's possible for synchronization primitives to remain alive even after being
+ * closed, because a thread is still waiting on them. It's rare in practice, and
+ * documented as being undefined behaviour by Microsoft, but it works, and some
+ * applications rely on it. This means we need to refcount handles, and defer
+ * deleting them on the server side until the refcount reaches zero. We do this
+ * by having each client process hold a handle to the in-process synchronization
+ * object, as well as a private refcount. When the client refcount reaches zero,
+ * it closes the handle; when all handles are closed, the server deletes the
+ * in-process synchronization object.
+ *
+ * We want lookup of objects from the cache to be very fast; ideally, it should
+ * be lock-free. We achieve this by using atomic modifications to "refcount",
+ * and guaranteeing that all other fields are valid and correct *as long as*
+ * refcount is nonzero, and we store the entire structure in memory which will
+ * never be freed.
+ *
+ * This means that acquiring the object can't use a simple atomic increment; it
+ * has to use a compare-and-swap loop to ensure that it doesn't try to increment
+ * an object with a zero refcount. That's still leagues better than a real lock,
+ * though, and release can be a single atomic decrement.
+ *
+ * It also means that threads modifying the cache need to take a lock, to
+ * prevent other threads from writing to it concurrently.
+ *
+ * It's possible for an object currently in use (by a waiter) to be closed and
+ * the same handle immediately reallocated to a different object. This should be
+ * a very rare situation, and in that case we simply don't cache the handle.
+ */
+struct inproc_sync_cache_entry
+{
+    LONG refcount;
+    int fd;
+    enum inproc_sync_type type;
+    unsigned int access;
+    BOOL closed;
+    /* handle to the underlying in-process sync object, stored as obj_handle_t
+     * to save space */
+    obj_handle_t handle;
+};
+
+
+static void release_inproc_sync_obj( struct inproc_sync_cache_entry *cache )
+{
+    /* save the handle and fd now; as soon as the refcount hits 0 we cannot
+     * access the cache anymore */
+    HANDLE handle = wine_server_ptr_handle( cache->handle );
+    int fd = cache->fd;
+    LONG refcount = InterlockedDecrement( &cache->refcount );
+
+    assert( refcount >= 0 );
+
+    if (!refcount)
+    {
+        NTSTATUS ret = NtClose( handle );
+        assert( !ret );
+        close( fd );
+    }
+}
+
+
+static BOOL inproc_sync_types_match( enum inproc_sync_type a, enum inproc_sync_type b )
+{
+    if (a == b) return TRUE;
+    if (a == INPROC_SYNC_AUTO_EVENT && b == INPROC_SYNC_MANUAL_EVENT) return TRUE;
+    if (b == INPROC_SYNC_AUTO_EVENT && a == INPROC_SYNC_MANUAL_EVENT) return TRUE;
+    return FALSE;
+}
+
+
+/* returns a pointer to a cache entry; if the object could not be cached,
+ * returns "stack_cache" instead, which should be allocated on stack */
+static NTSTATUS get_inproc_sync_obj( HANDLE handle, enum inproc_sync_type desired_type,
+                                     ACCESS_MASK desired_access,
+                                     struct inproc_sync_cache_entry *stack_cache,
+                                     struct inproc_sync_cache_entry **ret_cache )
+{
+    struct inproc_sync_cache_entry *cache = stack_cache;
+    int needs_close;
+    NTSTATUS ret;
+
+    *ret_cache = stack_cache;
+
+    SERVER_START_REQ( get_linux_sync_obj )
+    {
+        req->handle = wine_server_obj_handle( handle );
+        if (!(ret = wine_server_call( req )))
+        {
+            cache->handle = reply->handle;
+            cache->access = reply->access;
+            cache->type = reply->type;
+            cache->refcount = 1;
+            cache->closed = FALSE;
+        }
+    }
+    SERVER_END_REQ;
+
+    if (ret) return ret;
+
+    if ((ret = server_get_unix_fd( wine_server_ptr_handle( cache->handle ),
+                                   0, &cache->fd, &needs_close, NULL, NULL )))
+        return ret;
+
+    if (desired_type && !inproc_sync_types_match( cache->type, desired_type ))
+    {
+        release_inproc_sync_obj( cache );
+        return STATUS_OBJECT_TYPE_MISMATCH;
+    }
+
+    if ((cache->access & desired_access) != desired_access)
+    {
+        release_inproc_sync_obj( cache );
+        return STATUS_ACCESS_DENIED;
+    }
+
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS linux_release_semaphore_obj( int obj, ULONG count, ULONG *prev_count )
+{
+    NTSTATUS ret;
+
+    ret = ioctl( obj, NTSYNC_IOC_SEM_RELEASE, &count );
+    if (ret < 0)
+    {
+        if (errno == EOVERFLOW)
+            return STATUS_SEMAPHORE_LIMIT_EXCEEDED;
+        else
+            return errno_to_status( errno );
+    }
+    if (prev_count) *prev_count = count;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS inproc_release_semaphore( HANDLE handle, ULONG count, ULONG *prev_count )
+{
+    struct inproc_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+
+    if ((ret = get_inproc_sync_obj( handle, INPROC_SYNC_SEMAPHORE,
+                                    SEMAPHORE_MODIFY_STATE, &stack_cache, &cache )))
+        return ret;
+
+    ret = linux_release_semaphore_obj( cache->fd, count, prev_count );
+
+    release_inproc_sync_obj( cache );
+    return ret;
+}
+
+
+static NTSTATUS linux_query_semaphore_obj( int obj, SEMAPHORE_BASIC_INFORMATION *info )
+{
+    struct ntsync_sem_args args = {0};
+    NTSTATUS ret;
+
+    ret = ioctl( obj, NTSYNC_IOC_SEM_READ, &args );
+    if (ret < 0)
+        return errno_to_status( errno );
+    info->CurrentCount = args.count;
+    info->MaximumCount = args.max;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS inproc_query_semaphore( HANDLE handle, SEMAPHORE_BASIC_INFORMATION *info )
+{
+    struct inproc_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+
+    if ((ret = get_inproc_sync_obj( handle, INPROC_SYNC_SEMAPHORE,
+                                    SEMAPHORE_QUERY_STATE, &stack_cache, &cache )))
+        return ret;
+
+    ret = linux_query_semaphore_obj( cache->fd, info );
+
+    release_inproc_sync_obj( cache );
+    return ret;
+}
+
+
+static NTSTATUS linux_set_event_obj( int obj, LONG *prev_state )
+{
+    NTSTATUS ret;
+    __u32 prev;
+
+    ret = ioctl( obj, NTSYNC_IOC_EVENT_SET, &prev );
+    if (ret < 0)
+        return errno_to_status( errno );
+    if (prev_state) *prev_state = prev;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS inproc_set_event( HANDLE handle, LONG *prev_state )
+{
+    struct inproc_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+
+    if ((ret = get_inproc_sync_obj( handle, INPROC_SYNC_AUTO_EVENT,
+                                    EVENT_MODIFY_STATE, &stack_cache, &cache )))
+        return ret;
+
+    ret = linux_set_event_obj( cache->fd, prev_state );
+
+    release_inproc_sync_obj( cache );
+    return ret;
+}
+
+
+static NTSTATUS linux_reset_event_obj( int obj, LONG *prev_state )
+{
+    NTSTATUS ret;
+    __u32 prev;
+
+    ret = ioctl( obj, NTSYNC_IOC_EVENT_RESET, &prev );
+    if (ret < 0)
+        return errno_to_status( errno );
+    if (prev_state) *prev_state = prev;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS inproc_reset_event( HANDLE handle, LONG *prev_state )
+{
+    struct inproc_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+
+    if ((ret = get_inproc_sync_obj( handle, INPROC_SYNC_AUTO_EVENT,
+                                    EVENT_MODIFY_STATE, &stack_cache, &cache )))
+        return ret;
+
+    ret = linux_reset_event_obj( cache->fd, prev_state );
+
+    release_inproc_sync_obj( cache );
+    return ret;
+}
+
+
+static NTSTATUS linux_pulse_event_obj( int obj, LONG *prev_state )
+{
+    NTSTATUS ret;
+    __u32 prev;
+
+    ret = ioctl( obj, NTSYNC_IOC_EVENT_PULSE, &prev );
+    if (ret < 0)
+        return errno_to_status( errno );
+    if (prev_state) *prev_state = prev;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS inproc_pulse_event( HANDLE handle, LONG *prev_state )
+{
+    struct inproc_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+
+    if ((ret = get_inproc_sync_obj( handle, INPROC_SYNC_AUTO_EVENT,
+                                    EVENT_MODIFY_STATE, &stack_cache, &cache )))
+        return ret;
+
+    ret = linux_pulse_event_obj( cache->fd, prev_state );
+
+    release_inproc_sync_obj( cache );
+    return ret;
+}
+
+
+static NTSTATUS linux_query_event_obj( int obj, enum inproc_sync_type type, EVENT_BASIC_INFORMATION *info )
+{
+    struct ntsync_event_args args = {0};
+    NTSTATUS ret;
+
+    ret = ioctl( obj, NTSYNC_IOC_EVENT_READ, &args );
+    if (ret < 0)
+        return errno_to_status( errno );
+    info->EventType = (type == INPROC_SYNC_AUTO_EVENT) ? SynchronizationEvent : NotificationEvent;
+    info->EventState = args.signaled;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS inproc_query_event( HANDLE handle, EVENT_BASIC_INFORMATION *info )
+{
+    struct inproc_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+
+    if ((ret = get_inproc_sync_obj( handle, INPROC_SYNC_AUTO_EVENT,
+                                    EVENT_QUERY_STATE, &stack_cache, &cache )))
+        return ret;
+
+    ret = linux_query_event_obj( cache->fd, cache->type, info );
+
+    release_inproc_sync_obj( cache );
+    return ret;
+}
+
+
+static NTSTATUS linux_release_mutex_obj( int obj, LONG *prev_count )
+{
+    struct ntsync_mutex_args args = {0};
+    NTSTATUS ret;
+
+    args.owner = GetCurrentThreadId();
+    ret = ioctl( obj, NTSYNC_IOC_MUTEX_UNLOCK, &args );
+
+    if (ret < 0)
+    {
+        if (errno == EOVERFLOW)
+            return STATUS_MUTANT_LIMIT_EXCEEDED;
+        else if (errno == EPERM)
+            return STATUS_MUTANT_NOT_OWNED;
+        else
+            return errno_to_status( errno );
+    }
+    if (prev_count) *prev_count = 1 - args.count;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS inproc_release_mutex( HANDLE handle, LONG *prev_count )
+{
+    struct inproc_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+
+    if ((ret = get_inproc_sync_obj( handle, INPROC_SYNC_MUTEX, 0, &stack_cache, &cache )))
+        return ret;
+
+    ret = linux_release_mutex_obj( cache->fd, prev_count );
+
+    release_inproc_sync_obj( cache );
+    return ret;
+}
+
+
+static NTSTATUS linux_query_mutex_obj( int obj, MUTANT_BASIC_INFORMATION *info )
+{
+    struct ntsync_mutex_args args = {0};
+    NTSTATUS ret;
+
+    ret = ioctl( obj, NTSYNC_IOC_MUTEX_READ, &args );
+
+    if (ret < 0)
+    {
+        if (errno == EOWNERDEAD)
+        {
+            info->AbandonedState = TRUE;
+            info->OwnedByCaller = FALSE;
+            info->CurrentCount = 1;
+            return STATUS_SUCCESS;
+        }
+        else
+            return errno_to_status( errno );
+    }
+    info->AbandonedState = FALSE;
+    info->OwnedByCaller = (args.owner == GetCurrentThreadId());
+    info->CurrentCount = 1 - args.count;
+    return STATUS_SUCCESS;
+}
+
+
+static NTSTATUS inproc_query_mutex( HANDLE handle, MUTANT_BASIC_INFORMATION *info )
+{
+    struct inproc_sync_cache_entry stack_cache, *cache;
+    NTSTATUS ret;
+
+    if ((ret = get_inproc_sync_obj( handle, INPROC_SYNC_MUTEX, MUTANT_QUERY_STATE,
+                                    &stack_cache, &cache )))
+        return ret;
+
+    ret = linux_query_mutex_obj( cache->fd, info );
+
+    release_inproc_sync_obj( cache );
+    return ret;
+}
+
+static void select_queue(void)
+{
+    SERVER_START_REQ( select_inproc_queue )
+    {
+        wine_server_call( req );
+    }
+    SERVER_END_REQ;
+}
+
+static void unselect_queue( BOOL signaled )
+{
+    SERVER_START_REQ( unselect_inproc_queue )
+    {
+        req->signaled = signaled;
+        wine_server_call( req );
+    }
+    SERVER_END_REQ;
+}
+
+static int get_inproc_alert_obj(void)
+{
+    struct ntdll_thread_data *data = ntdll_get_thread_data();
+    struct inproc_sync_cache_entry stack_cache, *cache;
+    HANDLE alert_handle;
+    unsigned int ret;
+
+    if (data->linux_alert_obj == -1)
+    {
+        SERVER_START_REQ( get_inproc_alert_event )
+        {
+            if ((ret = wine_server_call( req )))
+                ERR( "failed to get inproc alert event, status %#x\n", ret );
+            alert_handle = wine_server_ptr_handle( reply->handle );
+        }
+        SERVER_END_REQ;
+
+        if ((ret = get_inproc_sync_obj( alert_handle, 0, SYNCHRONIZE, &stack_cache, &cache )))
+            ERR( "failed to get inproc alert obj, status %#x\n", ret );
+        data->linux_alert_obj = cache->fd;
+        /* Set the fd to -1 so release_inproc_sync_obj() won't close it.
+         * Manhandling the cache entry here is fine since we're the only thread
+         * that can access our own alert event. */
+        cache->fd = -1;
+        release_inproc_sync_obj( cache );
+        NtClose( alert_handle );
+    }
+
+    return data->linux_alert_obj;
+}
+
+static NTSTATUS linux_wait_objs( int device, const DWORD count, const int *objs,
+                                 BOOLEAN wait_any, BOOLEAN alertable, const LARGE_INTEGER *timeout )
+{
+    struct ntsync_wait_args args = {0};
+    unsigned long request;
+    struct timespec now;
+    int ret;
+
+    if (!timeout || timeout->QuadPart == TIMEOUT_INFINITE)
+    {
+        args.timeout = ~(__u64)0;
+    }
+    else if (timeout->QuadPart <= 0)
+    {
+        clock_gettime( CLOCK_MONOTONIC, &now );
+        args.timeout = (now.tv_sec * NSECPERSEC) + now.tv_nsec + (-timeout->QuadPart * 100);
+    }
+    else
+    {
+        args.timeout = (timeout->QuadPart * 100) - (SECS_1601_TO_1970 * NSECPERSEC);
+        args.flags |= NTSYNC_WAIT_REALTIME;
+    }
+
+    args.objs = (uintptr_t)objs;
+    args.count = count;
+    args.owner = GetCurrentThreadId();
+    args.index = ~0u;
+
+    if (alertable)
+        args.alert = get_inproc_alert_obj();
+
+    if (wait_any || count == 1)
+        request = NTSYNC_IOC_WAIT_ANY;
+    else
+        request = NTSYNC_IOC_WAIT_ALL;
+
+    do
+    {
+        ret = ioctl( device, request, &args );
+    } while (ret < 0 && errno == EINTR);
+
+    if (!ret)
+    {
+        if (args.index == count)
+        {
+            static const LARGE_INTEGER timeout;
+
+            ret = server_wait( NULL, 0, SELECT_INTERRUPTIBLE | SELECT_ALERTABLE, &timeout );
+            assert( ret == STATUS_USER_APC );
+            return ret;
+        }
+
+        return wait_any ? args.index : 0;
+    }
+    else if (errno == EOWNERDEAD)
+        return STATUS_ABANDONED + (wait_any ? args.index : 0);
+    else if (errno == ETIMEDOUT)
+        return STATUS_TIMEOUT;
+    else
+        return errno_to_status( errno );
+}
+
+static NTSTATUS inproc_wait( DWORD count, const HANDLE *handles, BOOLEAN wait_any,
+                             BOOLEAN alertable, const LARGE_INTEGER *timeout )
+{
+    struct inproc_sync_cache_entry stack_cache[64], *cache[64];
+    int device, objs[64];
+    HANDLE queue = NULL;
+    NTSTATUS ret;
+    DWORD i, j;
+
+    if ((device = get_linux_sync_device()) < 0)
+        return STATUS_NOT_IMPLEMENTED;
+
+    for (i = 0; i < count; ++i)
+    {
+        if ((ret = get_inproc_sync_obj( handles[i], 0, SYNCHRONIZE, &stack_cache[i], &cache[i] )))
+        {
+            for (j = 0; j < i; ++j)
+                release_inproc_sync_obj( cache[j] );
+            return ret;
+        }
+        if (cache[i]->type == INPROC_SYNC_QUEUE)
+            queue = handles[i];
+
+        objs[i] = cache[i]->fd;
+    }
+
+    if (queue) select_queue();
+
+    ret = linux_wait_objs( device, count, objs, wait_any, alertable, timeout );
+
+    if (queue) unselect_queue( handles[ret] == queue );
+
+    for (i = 0; i < count; ++i)
+        release_inproc_sync_obj( cache[i] );
+
+    return ret;
+}
+
+static NTSTATUS inproc_signal_and_wait( HANDLE signal, HANDLE wait,
+                                        BOOLEAN alertable, const LARGE_INTEGER *timeout )
+{
+    struct inproc_sync_cache_entry signal_stack_cache, *signal_cache;
+    struct inproc_sync_cache_entry wait_stack_cache, *wait_cache;
+    HANDLE queue = NULL;
+    NTSTATUS ret;
+    int device;
+
+    if ((device = get_linux_sync_device()) < 0)
+        return STATUS_NOT_IMPLEMENTED;
+
+    if ((ret = get_inproc_sync_obj( signal, 0, 0, &signal_stack_cache, &signal_cache )))
+        return ret;
+
+    switch (signal_cache->type)
+    {
+        case INPROC_SYNC_SEMAPHORE:
+            if (!(signal_cache->access & SEMAPHORE_MODIFY_STATE))
+            {
+                release_inproc_sync_obj( signal_cache );
+                return STATUS_ACCESS_DENIED;
+            }
+            break;
+
+        case INPROC_SYNC_AUTO_EVENT:
+        case INPROC_SYNC_MANUAL_EVENT:
+            if (!(signal_cache->access & EVENT_MODIFY_STATE))
+            {
+                release_inproc_sync_obj( signal_cache );
+                return STATUS_ACCESS_DENIED;
+            }
+            break;
+
+        case INPROC_SYNC_MUTEX:
+            break;
+
+        default:
+            /* can't be signaled */
+            release_inproc_sync_obj( signal_cache );
+            return STATUS_OBJECT_TYPE_MISMATCH;
+    }
+
+    if ((ret = get_inproc_sync_obj( wait, 0, SYNCHRONIZE, &wait_stack_cache, &wait_cache )))
+    {
+        release_inproc_sync_obj( signal_cache );
+        return ret;
+    }
+
+    if (wait_cache->type == INPROC_SYNC_QUEUE)
+        queue = wait;
+
+    switch (signal_cache->type)
+    {
+        case INPROC_SYNC_SEMAPHORE:
+            ret = linux_release_semaphore_obj( signal_cache->fd, 1, NULL );
+            break;
+
+        case INPROC_SYNC_AUTO_EVENT:
+        case INPROC_SYNC_MANUAL_EVENT:
+            ret = linux_set_event_obj( signal_cache->fd, NULL );
+            break;
+
+        case INPROC_SYNC_MUTEX:
+            ret = linux_release_mutex_obj( signal_cache->fd, NULL );
+            break;
+
+        default:
+            assert( 0 );
+            break;
+    }
+
+    if (!ret)
+    {
+        if (queue) select_queue();
+        ret = linux_wait_objs( device, 1, &wait_cache->fd, TRUE, alertable, timeout );
+        if (queue) unselect_queue( !ret );
+    }
+
+    release_inproc_sync_obj( signal_cache );
+    release_inproc_sync_obj( wait_cache );
+    return ret;
+}
+
+#else
+
+static NTSTATUS inproc_release_semaphore( HANDLE handle, ULONG count, ULONG *prev_count )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS inproc_query_semaphore( HANDLE handle, SEMAPHORE_BASIC_INFORMATION *info )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS inproc_set_event( HANDLE handle, LONG *prev_state )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS inproc_reset_event( HANDLE handle, LONG *prev_state )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS inproc_pulse_event( HANDLE handle, LONG *prev_state )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS inproc_query_event( HANDLE handle, EVENT_BASIC_INFORMATION *info )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS inproc_release_mutex( HANDLE handle, LONG *prev_count )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS inproc_query_mutex( HANDLE handle, MUTANT_BASIC_INFORMATION *info )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS inproc_wait( DWORD count, const HANDLE *handles, BOOLEAN wait_any,
+                             BOOLEAN alertable, const LARGE_INTEGER *timeout )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+static NTSTATUS inproc_signal_and_wait( HANDLE signal, HANDLE wait,
+                                        BOOLEAN alertable, const LARGE_INTEGER *timeout )
+{
+    return STATUS_NOT_IMPLEMENTED;
+}
+
+#endif
+
+
 /******************************************************************************
  *              NtCreateSemaphore (NTDLL.@)
  */
@@ -364,6 +1081,12 @@ NTSTATUS WINAPI NtQuerySemaphore( HANDLE handle, SEMAPHORE_INFORMATION_CLASS cla
     if (do_esync())
         return esync_query_semaphore( handle, info, ret_len );
 
+    if ((ret = inproc_query_semaphore( handle, out )) != STATUS_NOT_IMPLEMENTED)
+    {
+        if (!ret && ret_len) *ret_len = sizeof(SEMAPHORE_BASIC_INFORMATION);
+        return ret;
+    }
+
     SERVER_START_REQ( query_semaphore )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -394,6 +1117,9 @@ NTSTATUS WINAPI NtReleaseSemaphore( HANDLE handle, ULONG count, ULONG *previous
 
     TRACE( "handle %p, count %u, prev_count %p\n", handle, (int)count, previous );
 
+    if ((ret = inproc_release_semaphore( handle, count, previous )) != STATUS_NOT_IMPLEMENTED)
+        return ret;
+
     SERVER_START_REQ( release_semaphore )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -497,6 +1223,9 @@ NTSTATUS WINAPI NtSetEvent( HANDLE handle, LONG *prev_state )
 
     TRACE( "handle %p, prev_state %p\n", handle, prev_state );
 
+    if ((ret = inproc_set_event( handle, prev_state )) != STATUS_NOT_IMPLEMENTED)
+        return ret;
+
     SERVER_START_REQ( event_op )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -526,6 +1255,9 @@ NTSTATUS WINAPI NtResetEvent( HANDLE handle, LONG *prev_state )
 
     TRACE( "handle %p, prev_state %p\n", handle, prev_state );
 
+    if ((ret = inproc_reset_event( handle, prev_state )) != STATUS_NOT_IMPLEMENTED)
+        return ret;
+
     SERVER_START_REQ( event_op )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -563,6 +1295,9 @@ NTSTATUS WINAPI NtPulseEvent( HANDLE handle, LONG *prev_state )
 
     TRACE( "handle %p, prev_state %p\n", handle, prev_state );
 
+    if ((ret = inproc_pulse_event( handle, prev_state )) != STATUS_NOT_IMPLEMENTED)
+        return ret;
+
     SERVER_START_REQ( event_op )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -600,6 +1335,12 @@ NTSTATUS WINAPI NtQueryEvent( HANDLE handle, EVENT_INFORMATION_CLASS class,
     if (do_esync())
         return esync_query_event( handle, info, ret_len );
 
+    if ((ret = inproc_query_event( handle, out )) != STATUS_NOT_IMPLEMENTED)
+    {
+        if (!ret && ret_len) *ret_len = sizeof(EVENT_BASIC_INFORMATION);
+        return ret;
+    }
+
     SERVER_START_REQ( query_event )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -701,6 +1442,9 @@ NTSTATUS WINAPI NtReleaseMutant( HANDLE handle, LONG *prev_count )
 
     TRACE( "handle %p, prev_count %p\n", handle, prev_count );
 
+    if ((ret = inproc_release_mutex( handle, prev_count )) != STATUS_NOT_IMPLEMENTED)
+        return ret;
+
     SERVER_START_REQ( release_mutex )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -737,6 +1481,12 @@ NTSTATUS WINAPI NtQueryMutant( HANDLE handle, MUTANT_INFORMATION_CLASS class,
     if (do_esync())
         return esync_query_mutex( handle, info, ret_len );
 
+    if ((ret = inproc_query_mutex( handle, out )) != STATUS_NOT_IMPLEMENTED)
+    {
+        if (!ret && ret_len) *ret_len = sizeof(MUTANT_BASIC_INFORMATION);
+        return ret;
+    }
+
     SERVER_START_REQ( query_mutex )
     {
         req->handle = wine_server_obj_handle( handle );
@@ -1624,6 +2374,12 @@ NTSTATUS WINAPI NtWaitForMultipleObjects( DWORD count, const HANDLE *handles, BO
         TRACE( "}, timeout %s\n", debugstr_timeout(timeout) );
     }
 
+    if ((ret = inproc_wait( count, handles, wait_any, alertable, timeout )) != STATUS_NOT_IMPLEMENTED)
+    {
+        TRACE( "-> %#x\n", ret );
+        return ret;
+    }
+
     if (alertable) flags |= SELECT_ALERTABLE;
     select_op.wait.op = wait_any ? SELECT_WAIT : SELECT_WAIT_ALL;
     for (i = 0; i < count; i++) select_op.wait.handles[i] = wine_server_obj_handle( handles[i] );
@@ -1660,6 +2416,7 @@ NTSTATUS WINAPI NtSignalAndWaitForSingleObject( HANDLE signal, HANDLE wait,
 {
     union select_op select_op;
     UINT flags = SELECT_INTERRUPTIBLE;
+    NTSTATUS ret;
 
     if (do_fsync())
         return fsync_signal_and_wait( signal, wait, alertable, timeout );
@@ -1671,6 +2428,9 @@ NTSTATUS WINAPI NtSignalAndWaitForSingleObject( HANDLE signal, HANDLE wait,
 
     if (!signal) return STATUS_INVALID_HANDLE;
 
+    if ((ret = inproc_signal_and_wait( signal, wait, alertable, timeout )) != STATUS_NOT_IMPLEMENTED)
+        return ret;
+
     if (alertable) flags |= SELECT_ALERTABLE;
     select_op.signal_and_wait.op = SELECT_SIGNAL_AND_WAIT;
     select_op.signal_and_wait.wait = wine_server_obj_handle( wait );
diff --git a/dlls/ntdll/unix/unix_private.h b/dlls/ntdll/unix/unix_private.h
index 8c335cd1efd..02988a868f2 100644
--- a/dlls/ntdll/unix/unix_private.h
+++ b/dlls/ntdll/unix/unix_private.h
@@ -103,6 +103,7 @@ struct ntdll_thread_data
     PRTL_THREAD_START_ROUTINE start;         /* thread entry point */
     void                     *param;         /* thread entry point parameter */
     void                     *jmp_buf;       /* setjmp buffer for exception handling */
+    int                linux_alert_obj; /* fd for the linux in-process alert event */
 };
 
 C_ASSERT( sizeof(struct ntdll_thread_data) <= sizeof(((TEB *)0)->GdiTebBatch) );
@@ -392,6 +393,7 @@ extern void call_raise_user_exception_dispatcher(void);
 extern const char * wine_debuginfostr_pc(void *pc);
 
 #define TICKSPERSEC 10000000
+#define NSECPERSEC 1000000000
 #define SECS_1601_TO_1970  ((369 * 365 + 89) * (ULONGLONG)86400)
 
 static inline ULONGLONG ticks_from_time_t( time_t time )
diff --git a/dlls/ntdll/unix/virtual.c b/dlls/ntdll/unix/virtual.c
index 2af754736d1..46d5a76e090 100644
--- a/dlls/ntdll/unix/virtual.c
+++ b/dlls/ntdll/unix/virtual.c
@@ -4130,6 +4130,7 @@ static TEB *init_teb( void *ptr, BOOL is_wow )
     thread_data->reply_fd   = -1;
     thread_data->wait_fd[0] = -1;
     thread_data->wait_fd[1] = -1;
+    thread_data->linux_alert_obj = -1;
     list_add_head( &teb_list, &thread_data->entry );
     return teb;
 }
-- 
2.47.1


